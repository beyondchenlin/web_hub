"""
æµ‹è¯•è®ºå›åˆ†ç±»ä¿¡æ¯å­—æ®µ
åˆ†æä¸¤ä¸ªé¡µé¢çš„HTMLç»“æ„ï¼Œæ‰¾å‡ºå”¯ä¸€çš„åˆ†ç±»æ ‡è¯†

æ ¹æ®ç”¨æˆ·æˆªå›¾ï¼ŒDiscuzè®ºå›çš„åˆ†ç±»ä¿¡æ¯ï¼š
- åˆ¶ä½œAIå£°éŸ³: å˜é‡å myvoice
- éŸ³è‰²å…‹éš†: å˜é‡å clone
"""

import requests
from bs4 import BeautifulSoup
import re

# ğŸ” ä½¿ç”¨ç™»å½•å‡­è¯
USERNAME = "admin_ltcai"
PASSWORD = "Chenlin@2025"
BASE_URL = "https://tts.lrtcai.com"

def login_forum():
    """ç™»å½•è®ºå›"""
    session = requests.Session()

    try:
        print("ğŸ” ç™»å½•è®ºå›...")
        # è·å–ç™»å½•é¡µé¢
        login_page = session.get(f"{BASE_URL}/member.php?mod=logging&action=login", timeout=10)
        soup = BeautifulSoup(login_page.text, 'html.parser')

        # è·å–formhash
        form_hash = ""
        form_hash_input = soup.find('input', {'name': 'formhash'})
        if form_hash_input:
            form_hash = form_hash_input.get('value', '')

        # ç™»å½•æ•°æ®
        login_data = {
            'formhash': form_hash,
            'referer': BASE_URL,
            'loginfield': 'username',
            'username': USERNAME,
            'password': PASSWORD,
            'questionid': 0,
            'answer': '',
            'loginsubmit': 'true'
        }

        # å‘é€ç™»å½•è¯·æ±‚
        response = session.post(
            f"{BASE_URL}/member.php?mod=logging&action=login&loginsubmit=yes&infloat=yes&lssubmit=yes&inajax=1",
            data=login_data,
            allow_redirects=True,
            timeout=10
        )

        if response.status_code == 200:
            print("âœ… ç™»å½•æˆåŠŸ")
            return session
        else:
            print(f"âŒ ç™»å½•å¤±è´¥: {response.status_code}")
            return None

    except Exception as e:
        print(f"âŒ ç™»å½•å¼‚å¸¸: {e}")
        return None

def analyze_page(session, url, page_name):
    """åˆ†æé¡µé¢ç»“æ„"""
    print(f"\n{'='*60}")
    print(f"åˆ†æé¡µé¢: {page_name}")
    print(f"URL: {url}")
    print(f"{'='*60}")

    try:
        response = session.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ğŸ¯ é‡ç‚¹ï¼šæŸ¥æ‰¾åˆ†ç±»å˜é‡åï¼ˆmyvoice æˆ– cloneï¼‰
        print("\nğŸ¯ æŸ¥æ‰¾åˆ†ç±»å˜é‡åå’Œåˆ†ç±»ID:")

        html_text = response.text

        # æŸ¥æ‰¾ typeid æˆ– sortid å‚æ•°
        typeid_matches = re.findall(r'typeid[=\s]*["\']?(\d+)', html_text, re.I)
        sortid_matches = re.findall(r'sortid[=\s]*["\']?(\d+)', html_text, re.I)

        if typeid_matches:
            print(f"  âœ… æ‰¾åˆ° typeid: {set(typeid_matches)}")
        if sortid_matches:
            print(f"  âœ… æ‰¾åˆ° sortid: {set(sortid_matches)}")

        # æŸ¥æ‰¾åˆ†ç±»å˜é‡å
        if 'myvoice' in html_text.lower():
            print("  âœ… æ‰¾åˆ°å˜é‡å: myvoice â†’ åˆ¶ä½œAIå£°éŸ³")
            # æå–ç›¸å…³ä¸Šä¸‹æ–‡
            for line in html_text.split('\n'):
                if 'myvoice' in line.lower():
                    print(f"    {line.strip()[:200]}")
                    if 'typeid' in line.lower() or 'sortid' in line.lower():
                        break

        if 'clone' in html_text.lower():
            # æ’é™¤ voice_clone ç­‰å…¶ä»–åŒ…å«cloneçš„è¯
            for line in html_text.split('\n'):
                if re.search(r'\bclone\b', line.lower()) and ('typeid' in line.lower() or 'sortid' in line.lower() or 'sort' in line.lower()):
                    print("  âœ… æ‰¾åˆ°å˜é‡å: clone â†’ éŸ³è‰²å…‹éš†")
                    print(f"    {line.strip()[:200]}")
                    break

        # 1. æŸ¥æ‰¾captionå…ƒç´ 
        print("\nğŸ” 1. Captionå…ƒç´ :")
        captions = soup.find_all('caption')
        for i, caption in enumerate(captions):
            print(f"  Caption {i+1}: {caption.get_text(strip=True)}")
            print(f"    å±æ€§: {caption.attrs}")
        
        # 2. æŸ¥æ‰¾åˆ†ç±»ç›¸å…³çš„å…ƒç´ 
        print("\nğŸ” 2. åˆ†ç±»ä¿¡æ¯ (typeid/sortid):")
        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«typeidæˆ–sortidçš„å…ƒç´ 
        for tag in soup.find_all(attrs={'class': re.compile(r'.*type.*|.*sort.*|.*category.*', re.I)}):
            print(f"  æ ‡ç­¾: {tag.name}, class={tag.get('class')}, text={tag.get_text(strip=True)[:50]}")
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«typeidæˆ–sortidçš„å±æ€§
        for tag in soup.find_all(attrs=re.compile(r'typeid|sortid', re.I)):
            print(f"  æ ‡ç­¾: {tag.name}, å±æ€§={tag.attrs}")
        
        # 3. æŸ¥æ‰¾å¸–å­æ ‡é¢˜åŒºåŸŸ
        print("\nğŸ” 3. å¸–å­æ ‡é¢˜åŒºåŸŸ:")
        title_area = soup.find('div', id='pt')
        if title_area:
            print(f"  é¢åŒ…å±‘å¯¼èˆª: {title_area.get_text(strip=True)}")
        
        # 4. æŸ¥æ‰¾ä¸»é¢˜åˆ†ç±»æ ‡ç­¾
        print("\nğŸ” 4. ä¸»é¢˜åˆ†ç±»æ ‡ç­¾:")
        # Discuzé€šå¸¸åœ¨è¿™äº›ä½ç½®æ˜¾ç¤ºåˆ†ç±»
        for selector in ['span.tps', 'a.xi2', 'em.xi1', 'span[id^="thread_subject"]']:
            elements = soup.select(selector)
            for elem in elements:
                print(f"  {selector}: {elem.get_text(strip=True)}")
        
        # 5. æŸ¥æ‰¾å¸–å­è¯¦æƒ…åŒºåŸŸçš„åˆ†ç±»ä¿¡æ¯
        print("\nğŸ” 5. å¸–å­è¯¦æƒ…åŒºåŸŸ:")
        post_area = soup.find('div', class_='pct')
        if post_area:
            # æŸ¥æ‰¾æ‰€æœ‰emæ ‡ç­¾ï¼ˆé€šå¸¸ç”¨äºåˆ†ç±»æ ‡ç­¾ï¼‰
            em_tags = post_area.find_all('em')
            for em in em_tags:
                print(f"  <em>: {em.get_text(strip=True)}, class={em.get('class')}")
            
            # æŸ¥æ‰¾æ‰€æœ‰spanæ ‡ç­¾
            span_tags = post_area.find_all('span', class_=re.compile(r'.*'))
            for span in span_tags[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  <span>: {span.get_text(strip=True)[:30]}, class={span.get('class')}")
        
        # 6. æŸ¥æ‰¾è¡¨å•ä¸­çš„éšè—å­—æ®µ
        print("\nğŸ” 6. è¡¨å•éšè—å­—æ®µ:")
        hidden_inputs = soup.find_all('input', type='hidden')
        for inp in hidden_inputs:
            name = inp.get('name', '')
            value = inp.get('value', '')
            if 'type' in name.lower() or 'sort' in name.lower() or 'class' in name.lower():
                print(f"  {name} = {value}")
        
        # 7. æŸ¥æ‰¾JavaScriptä¸­çš„å˜é‡
        print("\nğŸ” 7. JavaScriptå˜é‡:")
        scripts = soup.find_all('script')
        for script in scripts:
            script_text = script.string or ''
            # æŸ¥æ‰¾typeidæˆ–sortidå˜é‡
            if 'typeid' in script_text.lower() or 'sortid' in script_text.lower():
                # æå–ç›¸å…³è¡Œ
                lines = script_text.split('\n')
                for line in lines:
                    if 'typeid' in line.lower() or 'sortid' in line.lower():
                        print(f"  {line.strip()[:100]}")
        
        # 8. æŸ¥æ‰¾å¸–å­å…ƒæ•°æ®
        print("\nğŸ” 8. å¸–å­å…ƒæ•°æ®:")
        # æŸ¥æ‰¾æ‰€æœ‰data-*å±æ€§
        for tag in soup.find_all(attrs=lambda x: x and any(k.startswith('data-') for k in x.keys())):
            data_attrs = {k: v for k, v in tag.attrs.items() if k.startswith('data-')}
            if data_attrs:
                print(f"  {tag.name}: {data_attrs}")
                break  # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ª
        
        # 9. æŸ¥æ‰¾å¸–å­ä¸»é¢˜åŒºåŸŸçš„æ‰€æœ‰class
        print("\nğŸ” 9. ä¸»é¢˜åŒºåŸŸçš„classå±æ€§:")
        main_content = soup.find('div', id='ct')
        if main_content:
            # æŸ¥æ‰¾æ‰€æœ‰å¸¦classçš„div
            divs_with_class = main_content.find_all('div', class_=True, limit=10)
            for div in divs_with_class:
                classes = div.get('class', [])
                text = div.get_text(strip=True)[:30]
                print(f"  class={classes}, text={text}")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # ç™»å½•è®ºå›
    session = login_forum()

    if session:
        # åˆ†æä¸¤ä¸ªé¡µé¢
        analyze_page(session, "https://tts.lrtcai.com/thread-22-1-1.html", "åˆ¶ä½œAIå£°éŸ³")
        analyze_page(session, "https://tts.lrtcai.com/thread-20-1-1.html", "éŸ³è‰²å…‹éš†")
    else:
        print("âŒ æ— æ³•ç™»å½•ï¼Œè·³è¿‡åˆ†æ")

