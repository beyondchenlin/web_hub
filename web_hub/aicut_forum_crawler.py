#!/usr/bin/env python3
# -*- encoding: utf-8 -*-

"""
æ‡’äººåŒåŸå·AI - æ™ºèƒ½å‰ªå£æ’­æ¿å—ä¸“ç”¨çˆ¬è™«

ä¸“é—¨ç›‘æ§: https://tts.lrtcai.com/forum-2-1.html
æ¿å—ID: 2
æ¿å—åç§°: æ™ºèƒ½å‰ªå£æ’­
"""

import os
import re
import json
import sys
import time
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import urllib.parse
from pathlib import Path

# ç¡®ä¿å¯ä»¥å¯¼å…¥ shared æ¨¡å—
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from shared.forum_config import load_forum_settings

# å°è¯•å¯¼å…¥ Seleniumï¼ˆå¯é€‰ï¼‰
try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.chrome.options import Options
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False


class AicutForumCrawler:
    """æ‡’äººåŒåŸå·AIè®ºå›çˆ¬è™« - ä¸“é—¨ç›‘æ§æ™ºèƒ½å‰ªå£æ’­æ¿å—"""

    def __init__(self, username: str = "", password: str = "", test_mode: bool = True, test_once: bool = False,
                 base_url: str = "", forum_url: str = ""):
        # ç»Ÿä¸€ä»é…ç½®æ–‡ä»¶åŠ è½½é»˜è®¤è®¾ç½®
        settings = load_forum_settings()
        forum_cfg = settings.get("forum", {})
        credentials_cfg = settings.get("credentials", {})

        # å…è®¸å¤–éƒ¨å‚æ•°æˆ–ç¯å¢ƒå˜é‡è¦†ç›–
        self.base_url = base_url or os.getenv('FORUM_BASE_URL') or forum_cfg["base_url"]
        self.forum_url = forum_url or os.getenv('FORUM_TARGET_URL') or forum_cfg["target_url"]

        self.username = (
            username or
            os.getenv('FORUM_USERNAME') or
            os.getenv('AICUT_ADMIN_USERNAME') or
            credentials_cfg.get("username", "")
        )
        self.password = (
            password or
            os.getenv('FORUM_PASSWORD') or
            os.getenv('AICUT_ADMIN_PASSWORD') or
            credentials_cfg.get("password", "")
        )

        # æ¨¡å¼é…ç½®
        self.test_mode = test_mode  # æµ‹è¯•æ¨¡å¼ï¼šé‡å¯åå¤„ç†æ‰€æœ‰å¸–å­ï¼›ç”Ÿäº§æ¨¡å¼ï¼šæŒä¹…åŒ–å»é‡
        self.test_once = test_once  # å•æ¬¡è¿è¡Œæ¨¡å¼ï¼šå¤„ç†ä¸€è½®ååœæ­¢
        self.processed_posts_file = "data/processed_posts.json"  # å·²å¤„ç†å¸–å­çš„æŒä¹…åŒ–æ–‡ä»¶

        # ä¼šè¯ç®¡ç†
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

        # ç¦ç”¨SSLéªŒè¯ä»¥é¿å…è¿æ¥é—®é¢˜
        self.session.verify = False

        # ç¦ç”¨SSLè­¦å‘Š
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

        self.logged_in = False
        self.last_check_time = 0
        self.processed_threads = set()  # å·²å¤„ç†çš„å¸–å­ID
        self.first_check_completed = False  # æ ‡è®°æ˜¯å¦å®Œæˆé¦–æ¬¡æ£€æŸ¥

        # åˆå§‹åŒ–å·²å¤„ç†å¸–å­åˆ—è¡¨
        self._load_processed_posts()

        print(f"ğŸ” åˆå§‹åŒ–æ™ºèƒ½å‰ªå£æ’­æ¿å—çˆ¬è™«")
        print(f"ğŸ“ ç›®æ ‡æ¿å—: {self.forum_url}")
        print(f"ğŸ›ï¸ è¿è¡Œæ¨¡å¼: {'ğŸ§ª æµ‹è¯•æ¨¡å¼' if self.test_mode else 'ğŸš€ ç”Ÿäº§æ¨¡å¼'}")
        if not self.test_mode:
            print(f"ğŸ’¾ å·²å¤„ç†å¸–å­æ•°: {len(self.processed_threads)}")

    def _load_processed_posts(self):
        """åŠ è½½å·²å¤„ç†çš„å¸–å­åˆ—è¡¨"""
        if self.test_mode:
            # æµ‹è¯•æ¨¡å¼ï¼šä¸åŠ è½½å†å²è®°å½•ï¼Œæ¯æ¬¡é‡å¯éƒ½æ˜¯å…¨æ–°å¼€å§‹
            self.processed_threads = set()
            print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šä¸åŠ è½½å†å²å¤„ç†è®°å½•")
            return

        # ç”Ÿäº§æ¨¡å¼ï¼šä»æ–‡ä»¶åŠ è½½å·²å¤„ç†çš„å¸–å­ID
        try:
            # ç¡®ä¿dataç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(self.processed_posts_file), exist_ok=True)

            if os.path.exists(self.processed_posts_file):
                with open(self.processed_posts_file, 'r', encoding='utf-8') as f:
                    processed_list = json.load(f)
                    self.processed_threads = set(processed_list)
                    print(f"ğŸ’¾ ç”Ÿäº§æ¨¡å¼ï¼šåŠ è½½äº† {len(self.processed_threads)} ä¸ªå·²å¤„ç†å¸–å­è®°å½•")
            else:
                self.processed_threads = set()
                print("ğŸ’¾ ç”Ÿäº§æ¨¡å¼ï¼šæœªæ‰¾åˆ°å†å²è®°å½•æ–‡ä»¶ï¼Œä»ç©ºå¼€å§‹")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å·²å¤„ç†å¸–å­è®°å½•å¤±è´¥: {e}")
            self.processed_threads = set()

    def _save_processed_posts(self):
        """ä¿å­˜å·²å¤„ç†çš„å¸–å­åˆ—è¡¨åˆ°æ–‡ä»¶"""
        if self.test_mode:
            # æµ‹è¯•æ¨¡å¼ï¼šä¸ä¿å­˜åˆ°æ–‡ä»¶
            return

        try:
            # ç¡®ä¿dataç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(self.processed_posts_file), exist_ok=True)

            with open(self.processed_posts_file, 'w', encoding='utf-8') as f:
                json.dump(list(self.processed_threads), f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å·²ä¿å­˜ {len(self.processed_threads)} ä¸ªå·²å¤„ç†å¸–å­è®°å½•")
        except Exception as e:
            print(f"âŒ ä¿å­˜å·²å¤„ç†å¸–å­è®°å½•å¤±è´¥: {e}")

    def mark_post_processed(self, post_id: str):
        """æ ‡è®°å¸–å­ä¸ºå·²å¤„ç†å¹¶ç«‹å³ä¿å­˜ï¼ˆç”Ÿäº§æ¨¡å¼ï¼‰"""
        self.processed_threads.add(post_id)

        if not self.test_mode:
            # ç”Ÿäº§æ¨¡å¼ï¼šç«‹å³ä¿å­˜åˆ°æ–‡ä»¶
            self._save_processed_posts()
    
    def login(self) -> bool:
        """ç™»å½•è®ºå›"""
        if not self.username or not self.password:
            print("âš ï¸ æœªæä¾›ç™»å½•ä¿¡æ¯ï¼Œä»¥æ¸¸å®¢æ¨¡å¼è¿è¡Œ")
            print(f"ğŸ” ç”¨æˆ·å: '{self.username}', å¯†ç : {'å·²è®¾ç½®' if self.password else 'æœªè®¾ç½®'}")
            return True

        try:
            print(f"ğŸ” å°è¯•ç™»å½•ç”¨æˆ·: {self.username}")
            print(f"ğŸŒ ç™»å½•URL: {self.base_url}")

            # é¦–å…ˆæµ‹è¯•åŸºç¡€è¿æ¥
            print("ğŸ”— æµ‹è¯•è®ºå›è¿æ¥...")
            test_response = self.session.get(self.base_url, timeout=10)
            print(f"âœ… è®ºå›è¿æ¥æˆåŠŸï¼ŒçŠ¶æ€ç : {test_response.status_code}")

            # è·å–ç™»å½•é¡µé¢
            print("ğŸ“„ è·å–ç™»å½•é¡µé¢...")
            login_page = self.session.get(f"{self.base_url}/member.php?mod=logging&action=login", timeout=10)
            print(f"ğŸ“„ ç™»å½•é¡µé¢çŠ¶æ€ç : {login_page.status_code}")

            soup = BeautifulSoup(login_page.text, 'html.parser')

            # æŸ¥æ‰¾ç™»å½•è¡¨å•çš„å¿…è¦å­—æ®µ
            form_hash = ""
            form_hash_input = soup.find('input', {'name': 'formhash'})
            if form_hash_input:
                form_hash = form_hash_input.get('value', '')
                print(f"ğŸ”‘ è·å–åˆ°formhash: {form_hash[:10]}...")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°formhashå­—æ®µ")

            # ç™»å½•æ•°æ®
            login_data = {
                'formhash': form_hash,
                'referer': self.base_url,
                'loginfield': 'username',
                'username': self.username,
                'password': self.password,
                'questionid': 0,
                'answer': '',
                'loginsubmit': 'true'
            }

            print("ğŸ“¤ å‘é€ç™»å½•è¯·æ±‚...")
            # å‘é€ç™»å½•è¯·æ±‚
            response = self.session.post(
                f"{self.base_url}/member.php?mod=logging&action=login&loginsubmit=yes&infloat=yes&lssubmit=yes&inajax=1",
                data=login_data,
                allow_redirects=True,
                timeout=10
            )

            print(f"ğŸ“¥ ç™»å½•å“åº”çŠ¶æ€ç : {response.status_code}")

            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥ç™»å½•æ˜¯å¦æˆåŠŸ
            response_text = response.text

            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯
            if 'å¯†ç é”™è¯¯' in response_text:
                print("âŒ ç™»å½•å¤±è´¥ï¼šå¯†ç é”™è¯¯")
                return False
            elif 'ç”¨æˆ·åä¸å­˜åœ¨' in response_text:
                print("âŒ ç™»å½•å¤±è´¥ï¼šç”¨æˆ·åä¸å­˜åœ¨")
                return False
            elif response.status_code == 503:
                print("âš ï¸ æœåŠ¡å™¨é™æµï¼ˆ503ï¼‰ï¼Œä½†å¯èƒ½å·²ç™»å½•")
                # æ£€æŸ¥cookiesåˆ¤æ–­æ˜¯å¦å·²ç™»å½•
                if any(cookie.name in ['cdb_sid', 'cdb_auth'] for cookie in self.session.cookies):
                    self.logged_in = True
                    print("âœ… æ£€æµ‹åˆ°ç™»å½•cookieï¼Œç™»å½•æˆåŠŸ")
                    return True
                return False

            # æ£€æŸ¥ç™»å½•æˆåŠŸçš„æ ‡å¿—
            # 1. é‡å®šå‘è„šæœ¬ï¼ˆDiscuzå¸¸è§çš„ç™»å½•æˆåŠŸå“åº”ï¼‰
            # 2. åŒ…å«ç”¨æˆ·å
            # 3. åŒ…å«ç™»å½•æˆåŠŸæç¤º
            # 4. æ£€æŸ¥cookie
            if ('window.location.href' in response_text or  # é‡å®šå‘è„šæœ¬
                'reload="1"' in response_text or  # é‡è½½æ ‡å¿—
                'ç™»å½•æˆåŠŸ' in response_text or
                self.username in response_text or
                any(cookie.name in ['cdb_sid', 'cdb_auth'] for cookie in self.session.cookies)):
                self.logged_in = True
                print("âœ… ç™»å½•æˆåŠŸ")
                return True
            else:
                print("âŒ ç™»å½•å¤±è´¥ï¼šæœªæ£€æµ‹åˆ°ç™»å½•æˆåŠŸæ ‡å¿—")
                print(f"å“åº”å†…å®¹å‰200å­—ç¬¦: {response_text[:200]}...")
                return False

        except Exception as e:
            print(f"âŒ ç™»å½•å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def get_forum_threads(self) -> List[Dict[str, Any]]:
        """è·å–æ™ºèƒ½å‰ªå£æ’­æ¿å—çš„æ‰€æœ‰å¸–å­"""
        try:
            print(f"ğŸ“‹ è·å–æ¿å—å¸–å­: {self.forum_url}")

            # è·å–æ¿å—é¡µé¢
            print("ğŸŒ è¯·æ±‚æ¿å—é¡µé¢...")
            response = self.session.get(self.forum_url, timeout=15)
            print(f"ğŸ“„ æ¿å—é¡µé¢çŠ¶æ€ç : {response.status_code}")
            response.raise_for_status()

            # ä¿å­˜é¡µé¢å†…å®¹ç”¨äºè°ƒè¯•
            page_content = response.text
            print(f"ğŸ“„ é¡µé¢å†…å®¹é•¿åº¦: {len(page_content)} å­—ç¬¦")

            soup = BeautifulSoup(page_content, 'html.parser')
            threads = []

            # æŸ¥æ‰¾å¸–å­åˆ—è¡¨ - å°è¯•å¤šç§é€‰æ‹©å™¨
            print("ğŸ” æŸ¥æ‰¾å¸–å­åˆ—è¡¨...")

            # æ–¹æ³•1: æŸ¥æ‰¾tbodyæ ‡ç­¾
            thread_rows = soup.find_all('tbody')
            print(f"ğŸ” æ‰¾åˆ° {len(thread_rows)} ä¸ªtbodyå…ƒç´ ")

            # æ–¹æ³•2: å¦‚æœtbodyæ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
            if not thread_rows:
                thread_rows = soup.find_all('tr')
                print(f"ğŸ” å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰¾åˆ° {len(thread_rows)} ä¸ªtrå…ƒç´ ")

            # æ–¹æ³•3: æŸ¥æ‰¾åŒ…å«threadé“¾æ¥çš„å…ƒç´ 
            if not thread_rows:
                thread_links = soup.find_all('a', href=re.compile(r'thread-\d+-\d+-\d+\.html'))
                print(f"ğŸ” ç›´æ¥æŸ¥æ‰¾ï¼šæ‰¾åˆ° {len(thread_links)} ä¸ªthreadé“¾æ¥")
                # å°†é“¾æ¥è½¬æ¢ä¸ºè¡Œæ ¼å¼
                thread_rows = [link.parent for link in thread_links if link.parent]

            processed_thread_ids = set()  # é¿å…é‡å¤å¤„ç†

            for i, row in enumerate(thread_rows):
                try:
                    # æŸ¥æ‰¾å¸–å­é“¾æ¥ - ä¼˜å…ˆæŸ¥æ‰¾å¸¦æ ‡é¢˜çš„é“¾æ¥ï¼ˆclass="xst"ï¼‰
                    thread_link = row.find('a', class_='xst', href=re.compile(r'thread-\d+-\d+-\d+\.html'))

                    # å¦‚æœæ²¡æ‰¾åˆ°ï¼ŒæŸ¥æ‰¾æ‰€æœ‰threadé“¾æ¥ï¼Œé€‰æ‹©æœ‰æ–‡æœ¬çš„
                    if not thread_link:
                        all_thread_links = row.find_all('a', href=re.compile(r'thread-\d+-\d+-\d+\.html'))
                        for link in all_thread_links:
                            if link.get_text(strip=True):
                                thread_link = link
                                break

                    # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªthreadé“¾æ¥
                    if not thread_link:
                        thread_link = row.find('a', href=re.compile(r'thread-\d+-\d+-\d+\.html'))

                    if not thread_link:
                        continue

                    # æå–å¸–å­ä¿¡æ¯
                    thread_url = thread_link.get('href')
                    if not thread_url.startswith('http'):
                        thread_url = self.base_url + '/' + thread_url.lstrip('/')

                    # æå–å¸–å­ID
                    thread_id_match = re.search(r'thread-(\d+)-', thread_url)
                    if not thread_id_match:
                        continue

                    thread_id = thread_id_match.group(1)

                    # é¿å…é‡å¤å¤„ç†
                    if thread_id in processed_thread_ids:
                        continue
                    processed_thread_ids.add(thread_id)

                    # è·å–å¸–å­æ ‡é¢˜
                    title = thread_link.get_text(strip=True)

                    # å¦‚æœæ ‡é¢˜ä¸ºç©ºï¼Œå°è¯•ä»å…¶ä»–threadé“¾æ¥è·å–
                    if not title:
                        all_thread_links = row.find_all('a', href=re.compile(r'thread-\d+-\d+-\d+\.html'))
                        for link in all_thread_links:
                            link_text = link.get_text(strip=True)
                            if link_text:
                                title = link_text
                                break

                    # æŸ¥æ‰¾ä½œè€…ä¿¡æ¯
                    author_link = row.find('a', href=re.compile(r'space-uid-\d+\.html'))
                    author = author_link.get_text(strip=True) if author_link else "æœªçŸ¥ç”¨æˆ·"

                    # æŸ¥æ‰¾å‘å¸–æ—¶é—´
                    time_elements = row.find_all('em')
                    post_time = ""
                    for elem in time_elements:
                        text = elem.get_text(strip=True)
                        if 'å°æ—¶å‰' in text or 'åˆ†é’Ÿå‰' in text or 'å¤©å‰' in text or '-' in text:
                            post_time = text
                            break

                    thread_info = {
                        'thread_id': thread_id,
                        'title': title,
                        'author': author,
                        'thread_url': thread_url,
                        'post_time': post_time,
                        'forum_id': 2,
                        'forum_name': 'æ™ºèƒ½å‰ªå£æ’­'
                    }

                    threads.append(thread_info)
                    print(f"ğŸ“ å‘ç°å¸–å­ {len(threads)}: {title} (ID: {thread_id}) - ä½œè€…: {author}")

                except Exception as e:
                    print(f"âš ï¸ è§£æç¬¬ {i+1} ä¸ªå¸–å­è¡Œå¤±è´¥: {e}")
                    continue

            print(f"ğŸ“Š å…±å‘ç° {len(threads)} ä¸ªå¸–å­")

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¸–å­ï¼Œè¾“å‡ºè°ƒè¯•ä¿¡æ¯
            if not threads:
                print("ğŸ” æœªæ‰¾åˆ°å¸–å­ï¼Œè¾“å‡ºé¡µé¢è°ƒè¯•ä¿¡æ¯...")
                print(f"é¡µé¢æ ‡é¢˜: {soup.title.get_text() if soup.title else 'æ— æ ‡é¢˜'}")
                # æŸ¥æ‰¾å¯èƒ½çš„é”™è¯¯ä¿¡æ¯
                error_divs = soup.find_all('div', class_=['error', 'message'])
                for error_div in error_divs:
                    print(f"é”™è¯¯ä¿¡æ¯: {error_div.get_text(strip=True)}")

                # è¾“å‡ºé¡µé¢çš„å‰1000ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
                print("é¡µé¢å†…å®¹é¢„è§ˆ:")
                print(page_content[:1000])
                print("..." if len(page_content) > 1000 else "")

            return threads

        except Exception as e:
            print(f"âŒ è·å–æ¿å—å¸–å­å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def get_thread_content(self, thread_url: str) -> Dict[str, Any]:
        """è·å–å¸–å­è¯¦ç»†å†…å®¹"""
        try:
            print(f"ğŸ“– è·å–å¸–å­å†…å®¹: {thread_url}")
            
            response = self.session.get(thread_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾å¸–å­å†…å®¹ - å°è¯•å¤šç§é€‰æ‹©å™¨ä»¥è·å–å®Œæ•´å†…å®¹
            content = ""

            # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨ï¼Œä¼˜å…ˆé€‰æ‹©åŒ…å«æ›´å¤šä¿¡æ¯çš„
            content_selectors = [
                # æœ€ä½³é€‰æ‹©å™¨ï¼šåŒ…å«å®Œæ•´å†…å®¹å’Œå°é¢æ ‡é¢˜
                'div.pct',
                # å®Œæ•´å¸–å­å†…å®¹åŒºåŸŸ
                'div.postmessage',
                'div.t_fsz',
                'td.t_f',
                # æ›´å¹¿æ³›çš„å†…å®¹åŒºåŸŸ
                'div.plhin',
                # å¤‡ç”¨é€‰æ‹©å™¨
                'div[id^="postmessage_"]',
                'td[id^="postmessage_"]'
            ]

            for selector in content_selectors:
                content_div = soup.select_one(selector)
                if content_div:
                    content = content_div.get_text(separator='\n', strip=True)
                    print(f"ğŸ“„ ä½¿ç”¨é€‰æ‹©å™¨æå–å†…å®¹: {selector} (é•¿åº¦: {len(content)})")
                    break

            # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•ä»æ•´ä¸ªé¡µé¢æå–
            if not content:
                # æŸ¥æ‰¾åŒ…å«å¸–å­å†…å®¹çš„ä¸»è¦åŒºåŸŸ
                main_content = soup.find('div', {'id': 'ct'}) or soup.find('div', class_='wp')
                if main_content:
                    content = main_content.get_text(separator='\n', strip=True)
                    print(f"ğŸ“„ ä½¿ç”¨ä¸»è¦åŒºåŸŸæå–å†…å®¹ (é•¿åº¦: {len(content)})")
                else:
                    # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
                    content = soup.get_text(separator='\n', strip=True)
                    print(f"ğŸ“„ ä½¿ç”¨æ•´é¡µå†…å®¹æå– (é•¿åº¦: {len(content)})")
            
            # æŸ¥æ‰¾è§†é¢‘é“¾æ¥å’Œæ–‡ä»¶å
            video_urls, original_filenames = self._extract_video_urls_and_names(str(soup))

            # æŸ¥æ‰¾éŸ³é¢‘é“¾æ¥
            audio_urls = self._extract_audio_urls(str(soup))

            # æŸ¥æ‰¾é™„ä»¶
            attachments = self._extract_attachments(soup)

            # æå–å°é¢ä¿¡æ¯
            cover_info = self._extract_cover_info(content)

            # ç»“æ„åŒ–å†…å®¹å¤„ç†
            structured_content = self._process_structured_content(content)

            return {
                'content': content,                                    # åŸå§‹å†…å®¹
                'structured_content': structured_content,             # ç»“æ„åŒ–å†…å®¹
                'core_text': structured_content.get('core_text', ''), # æ ¸å¿ƒæ–‡æœ¬ï¼ˆç”¨äºçƒ­è¯ï¼‰
                'video_urls': video_urls,
                'original_filenames': original_filenames,
                'audio_urls': audio_urls,
                'attachments': attachments,
                'cover_info': cover_info,
                'has_video': len(video_urls) > 0 or len(attachments) > 0,
                'has_audio': len(audio_urls) > 0
            }
            
        except Exception as e:
            print(f"âŒ è·å–å¸–å­å†…å®¹å¤±è´¥: {e}")
            return {
                'content': "",
                'video_urls': [],
                'audio_urls': [],
                'attachments': [],
                'cover_info': {},
                # ğŸ¯ æºå¤´ä¿®å¤ï¼šé”™è¯¯æƒ…å†µä¸‹ä¹Ÿæä¾›ç©ºçš„å°é¢æ ‡é¢˜å­—æ®µ
                'cover_title_up': '',
                'cover_title_down': '',
                'has_video': False,
                'has_audio': False
            }
    
    def _extract_video_urls_and_names(self, html_content: str) -> Tuple[List[str], List[str]]:
        """ä»HTMLå†…å®¹ä¸­æå–è§†é¢‘é“¾æ¥å’Œå¯¹åº”çš„æ–‡ä»¶å"""
        video_urls = []
        video_names = []

        # é¦–å…ˆå°è¯•è§£æHTML <a> æ ‡ç­¾æ ¼å¼: <a href="é“¾æ¥">æ–‡ä»¶å</a>
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')

        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«è§†é¢‘é“¾æ¥çš„ <a> æ ‡ç­¾
        video_links = soup.find_all('a', href=re.compile(r'https?://[^"\']*\.(?:mp4|avi|mov|mkv|flv|wmv|webm)', re.IGNORECASE))

        for link in video_links:
            url = link.get('href')
            filename = link.get_text(strip=True)

            if url and filename:
                video_urls.append(url)
                # æ¸…ç†æ–‡ä»¶åï¼Œç¡®ä¿æœ‰æ­£ç¡®çš„æ‰©å±•å
                clean_filename = filename.strip()
                if not any(clean_filename.lower().endswith(ext) for ext in ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.webm']):
                    clean_filename += '.mp4'
                video_names.append(clean_filename)
                print(f"ğŸ“ HTMLé“¾æ¥è§£æ: {url} -> {clean_filename}")

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°HTMLé“¾æ¥ï¼Œå°è¯•è§£æBBCodeæ ¼å¼çš„é“¾æ¥: [url=é“¾æ¥]æ–‡ä»¶å[/url]
        if not video_urls:
            bbcode_pattern = r'\[url=(https?://[^\]]+\.(?:mp4|avi|mov|mkv|flv|wmv|webm)[^\]]*)\]([^[]+)\[/url\]'
            bbcode_matches = re.findall(bbcode_pattern, html_content, re.IGNORECASE)

            for url, filename in bbcode_matches:
                video_urls.append(url)
                # æ¸…ç†æ–‡ä»¶åï¼Œç¡®ä¿æœ‰æ­£ç¡®çš„æ‰©å±•å
                clean_filename = filename.strip()
                if not any(clean_filename.lower().endswith(ext) for ext in ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.webm']):
                    clean_filename += '.mp4'
                video_names.append(clean_filename)
                print(f"ğŸ“ BBCodeè§£æ: {url} -> {clean_filename}")

        # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„URLæå–æ–¹å¼
        if not video_urls:
            # è§†é¢‘URLæ¨¡å¼ - é’ˆå¯¹æ‚¨ç½‘ç«™çš„è…¾è®¯äº‘COSå­˜å‚¨
            patterns = [
                # è…¾è®¯äº‘COSè§†é¢‘é“¾æ¥ (æ‚¨ç½‘ç«™ä½¿ç”¨çš„å­˜å‚¨)
                r'https?://lrtcai-\d+\.cos\.ap-[^/]+\.myqcloud\.com/[^\s<>"\']*\.(?:mp4|avi|mov|mkv|flv|wmv|webm)',
                # é€šç”¨ç›´é“¾è§†é¢‘
                r'https?://[^\s<>"\']+\.(?:mp4|avi|mov|mkv|flv|wmv|webm)',
                # è§†é¢‘å¹³å°é“¾æ¥
                r'https?://[^\s<>"\']*(?:youtube|youtu\.be|bilibili|douyin)[^\s<>"\']*',
                # ç½‘ç›˜é“¾æ¥
                r'https?://[^\s<>"\']*(?:pan\.baidu|aliyundrive|123pan)[^\s<>"\']*',
            ]

            for pattern in patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                video_urls.extend(matches)

            # å»é‡å¹¶è¿‡æ»¤
            unique_urls = list(set(video_urls))

            # è¿‡æ»¤æ‰éŸ³é¢‘æ–‡ä»¶ï¼ˆ.mp3ç­‰ï¼‰
            video_only_urls = []
            for url in unique_urls:
                if not any(url.lower().endswith(ext) for ext in ['.mp3', '.wav', '.aac', '.flac']):
                    video_only_urls.append(url)

            video_urls = video_only_urls
            # å¯¹äºä¼ ç»Ÿæ–¹å¼æå–çš„URLï¼Œä»URLä¸­æå–æ–‡ä»¶å
            video_names = [self._extract_filename_from_url(url) for url in video_urls]

        return video_urls, video_names

    def _extract_video_urls(self, html_content: str) -> List[str]:
        """ä»HTMLå†…å®¹ä¸­æå–è§†é¢‘é“¾æ¥ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        video_urls, _ = self._extract_video_urls_and_names(html_content)
        return video_urls

    def _extract_filename_from_url(self, url: str) -> str:
        """ä»URLä¸­æå–åŸå§‹æ–‡ä»¶åï¼Œä¿æŒä¸­æ–‡å­—ç¬¦"""
        try:
            import urllib.parse
            import os

            # è§£æURL
            parsed_url = urllib.parse.urlparse(url)

            # ä»è·¯å¾„ä¸­æå–æ–‡ä»¶å
            path = parsed_url.path
            if not path:
                return ""

            # è·å–è·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†ï¼ˆæ–‡ä»¶åï¼‰
            filename = os.path.basename(path)

            if not filename:
                return ""

            # URLè§£ç ï¼Œå¤„ç†ä¸­æ–‡å­—ç¬¦
            filename = urllib.parse.unquote(filename, encoding='utf-8')

            # éªŒè¯æ–‡ä»¶åæ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
            video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.webm']
            if not any(filename.lower().endswith(ext) for ext in video_extensions):
                # å¦‚æœæ²¡æœ‰è§†é¢‘æ‰©å±•åï¼Œæ·»åŠ .mp4
                if '.' not in filename:
                    filename += '.mp4'
                else:
                    # æ›¿æ¢æ‰©å±•åä¸º.mp4
                    name_without_ext = os.path.splitext(filename)[0]
                    filename = name_without_ext + '.mp4'

            # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ï¼‰
            # ç§»é™¤Windowsæ–‡ä»¶åä¸­ä¸å…è®¸çš„å­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡
            illegal_chars = r'[<>:"/\\|?*]'
            filename = re.sub(illegal_chars, '_', filename)

            # é™åˆ¶æ–‡ä»¶åé•¿åº¦
            if len(filename) > 200:
                name_part, ext_part = os.path.splitext(filename)
                max_name_length = 200 - len(ext_part)
                filename = name_part[:max_name_length] + ext_part

            return filename

        except Exception as e:
            print(f"âš ï¸ æ— æ³•ä»URLæå–æ–‡ä»¶å: {e}")
            return ""

    def extract_original_filenames(self, video_urls: List[str], html_content: str = None) -> List[str]:
        """æå–è§†é¢‘URLåˆ—è¡¨å¯¹åº”çš„åŸå§‹æ–‡ä»¶ååˆ—è¡¨"""
        if html_content:
            # å¦‚æœæä¾›äº†HTMLå†…å®¹ï¼Œé‡æ–°è§£æä»¥è·å–å‡†ç¡®çš„æ–‡ä»¶å
            urls, filenames = self._extract_video_urls_and_names(html_content)
            # è¿”å›ä¸æä¾›çš„video_urlsåŒ¹é…çš„æ–‡ä»¶å
            result_filenames = []
            for url in video_urls:
                if url in urls:
                    idx = urls.index(url)
                    result_filenames.append(filenames[idx])
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šä»URLæå–
                    filename = self._extract_filename_from_url(url)
                    result_filenames.append(filename if filename else f"video_{len(result_filenames)+1}.mp4")
            return result_filenames
        else:
            # ä¼ ç»Ÿæ–¹å¼ï¼šä»URLæå–æ–‡ä»¶å
            filenames = []
            for url in video_urls:
                filename = self._extract_filename_from_url(url)
                if filename:
                    filenames.append(filename)
                    print(f"ğŸ“ æå–æ–‡ä»¶å: {url} -> {filename}")
                else:
                    # å¦‚æœæ— æ³•æå–ï¼Œä½¿ç”¨URLçš„æœ€åéƒ¨åˆ†ä½œä¸ºå¤‡ç”¨
                    import os
                    backup_name = os.path.basename(url.split('?')[0])  # ç§»é™¤æŸ¥è¯¢å‚æ•°
                    if not backup_name.endswith('.mp4'):
                        backup_name += '.mp4'
                    filenames.append(backup_name)
                    print(f"ğŸ“ å¤‡ç”¨æ–‡ä»¶å: {url} -> {backup_name}")
            return filenames

    def _extract_audio_urls(self, html_content: str) -> List[str]:
        """ä»HTMLå†…å®¹ä¸­æå–éŸ³é¢‘é“¾æ¥"""
        audio_urls = []

        # éŸ³é¢‘URLæ¨¡å¼ - é’ˆå¯¹æ‚¨ç½‘ç«™çš„è…¾è®¯äº‘COSå­˜å‚¨
        patterns = [
            # è…¾è®¯äº‘COSéŸ³é¢‘é“¾æ¥ï¼ˆå¢åŠ amræ ¼å¼æ”¯æŒï¼‰
            r'https?://lrtcai-\d+\.cos\.ap-[^/]+\.myqcloud\.com/[^\s<>"\']*\.(?:mp3|wav|aac|flac|m4a|amr)',
            # é€šç”¨éŸ³é¢‘é“¾æ¥ï¼ˆå¢åŠ amræ ¼å¼æ”¯æŒï¼‰
            r'https?://[^\s<>"\']+\.(?:mp3|wav|aac|flac|m4a|amr)',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            audio_urls.extend(matches)

        # å»é‡
        return list(set(audio_urls))

    def _extract_cover_info(self, content: str) -> Dict[str, str]:
        """æå–å°é¢ä¿¡æ¯ - ä½¿ç”¨ç»Ÿä¸€çš„up/downå‡½æ•°"""
        cover_info = {}

        try:
            # ğŸ¯ ä½¿ç”¨ç»Ÿä¸€çš„å°é¢æ ‡é¢˜æå–å‡½æ•°ï¼ˆè§†é¢‘å¤„ç†æ¨¡å—ï¼ŒTTSç³»ç»Ÿå¯é€‰ï¼‰
            from pre.stage.unified_content_processor import extract_cover_title_up, extract_cover_title_middle, extract_cover_title_down

            # æå–å°é¢æ ‡é¢˜ä¸Šã€ä¸­ã€ä¸‹
            cover_title_up = extract_cover_title_up(content)
            cover_title_middle = extract_cover_title_middle(content)
            cover_title_down = extract_cover_title_down(content)

            # ğŸ¯ ä½¿ç”¨ç»Ÿä¸€çš„up/middle/downå­—æ®µåï¼Œåªä¿å­˜å’Œæ˜¾ç¤ºæœ‰å†…å®¹çš„æ ‡é¢˜
            extracted_titles = []

            if cover_title_up:
                cover_info['cover_title_up'] = cover_title_up
                extracted_titles.append(f"ä¸Šæ ‡é¢˜: '{cover_title_up}'")

            if cover_title_middle:
                cover_info['cover_title_middle'] = cover_title_middle
                extracted_titles.append(f"ä¸­æ ‡é¢˜: '{cover_title_middle}'")

            if cover_title_down:
                cover_info['cover_title_down'] = cover_title_down
                extracted_titles.append(f"ä¸‹æ ‡é¢˜: '{cover_title_down}'")

            # ç»Ÿä¸€æ˜¾ç¤ºæå–åˆ°çš„æ ‡é¢˜
            if extracted_titles:
                print("ğŸ“ æå–åˆ°çš„å°é¢æ ‡é¢˜:")
                for title in extracted_titles:
                    print(f"   {title}")
        except ImportError:
            # TTSç³»ç»Ÿä¸éœ€è¦è§†é¢‘å¤„ç†æ¨¡å—ï¼Œè·³è¿‡å°é¢æ ‡é¢˜æå–
            pass

        return cover_info

    def _process_structured_content(self, content: str) -> Dict[str, Any]:
        """å¤„ç†ç»“æ„åŒ–å†…å®¹"""
        try:
            # å¯¼å…¥å†…å®¹å¤„ç†å™¨
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.abspath(__file__)))

            try:
                from pre.stage.unified_content_processor import process_forum_content_unified

                # ä½¿ç”¨ç»Ÿä¸€ç»“æ„åŒ–å¤„ç†å™¨
                structured = process_forum_content_unified(content)

                return {
                    'core_text': structured.core_text,
                    'system_tags': structured.system_tags or [],
                    'cover_title_up': structured.cover_title_up,
                    'cover_title_middle': structured.cover_title_middle,
                    'cover_title_down': structured.cover_title_down,
                    'urls': structured.urls or [],
                    'bbcode_tags': structured.bbcode_tags or [],
                    'content_type': structured.content_type,
                    'has_media_content': structured.has_media_content,
                    'original_length': structured.original_length,
                    'core_text_length': structured.core_text_length,
                    'filtered_elements_count': structured.filtered_elements_count
                }

            except ImportError:
                print("âš ï¸ ç»Ÿä¸€å†…å®¹å¤„ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€å¤„ç†")
                return self._basic_content_processing(content)

        except Exception as e:
            print(f"âŒ ç»“æ„åŒ–å†…å®¹å¤„ç†å¤±è´¥: {e}")
            return self._basic_content_processing(content)

    def _basic_content_processing(self, content: str) -> Dict[str, Any]:
        """åŸºç¡€å†…å®¹å¤„ç†ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        import re

        # åŸºç¡€æ¸…ç†
        core_text = content

        # ç§»é™¤ç³»ç»Ÿæ ‡è¯†
        core_text = re.sub(r'æ‡’äººæ™ºèƒ½å‰ªè¾‘\s*', '', core_text)

        # ç§»é™¤å°é¢ä¿¡æ¯
        core_text = re.sub(r'å°é¢æ ‡é¢˜[ä¸Šä¸­ä¸‹]?\s*[:ï¼š]\s*[^\n]*', '', core_text)

        # ç§»é™¤é“¾æ¥
        core_text = re.sub(r'https?://[^\s]+', '', core_text)
        core_text = re.sub(r'\[url[^\]]*\].*?\[/url\]', '', core_text, flags=re.IGNORECASE)

        # æ¸…ç†ç©ºæ ¼
        core_text = re.sub(r'\s+', ' ', core_text).strip()

        return {
            'core_text': core_text,
            'system_tags': [],
            'cover_title_up': '',
            'cover_title_down': '',
            'urls': [],
            'bbcode_tags': [],
            'content_type': 'text_only',
            'has_media_content': False,
            'original_length': len(content),
            'core_text_length': len(core_text),
            'filtered_elements_count': 0
        }

    def _extract_attachments(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """æå–é™„ä»¶ä¿¡æ¯"""
        attachments = []
        
        # æŸ¥æ‰¾é™„ä»¶é“¾æ¥
        attach_links = soup.find_all('a', href=re.compile(r'attachment\.php'))
        
        for link in attach_links:
            attach_url = link.get('href')
            if not attach_url.startswith('http'):
                attach_url = self.base_url + '/' + attach_url.lstrip('/')
            
            attach_name = link.get_text(strip=True)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
            if any(ext in attach_name.lower() for ext in ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.webm']):
                attachments.append({
                    'name': attach_name,
                    'url': attach_url,
                    'type': 'video'
                })
        
        return attachments
    
    def monitor_new_posts(self) -> List[Dict[str, Any]]:
        """ç›‘æ§æ–°å¸–å­ - æ™ºèƒ½æ¨¡å¼åˆ‡æ¢ç‰ˆæœ¬"""
        try:
            print(f"ğŸ” å¼€å§‹ç›‘æ§æ™ºèƒ½å‰ªå£æ’­æ¿å— ({datetime.now().strftime('%H:%M:%S')})")

            # è·å–æ‰€æœ‰å¸–å­
            threads = self.get_forum_threads()

            new_video_posts = []

            # æµ‹è¯•æ¨¡å¼ vs ç”Ÿäº§æ¨¡å¼çš„ä¸åŒå¤„ç†é€»è¾‘
            if self.test_mode:
                # ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰å¸–å­ï¼ˆåŒ…æ‹¬å·²å¤„ç†è¿‡çš„ï¼‰
                print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šæ£€æŸ¥æ‰€æœ‰å¸–å­")
                for thread in threads:
                    thread_id = thread['thread_id']

                    print(f"ğŸ” æ£€æŸ¥å¸–å­: {thread['title']} (ID: {thread_id})")

                    # è·å–å¸–å­è¯¦ç»†å†…å®¹
                    thread_content = self.get_thread_content(thread['thread_url'])

                    # ğŸ¯ æ”¯æŒä¸‰ç§ç±»å‹çš„å¸–å­ï¼š
                    # 1. è§†é¢‘å¸–å­ï¼ˆè§†é¢‘å¤„ç†ï¼‰
                    # 2. éŸ³é¢‘å¸–å­ï¼ˆéŸ³è‰²å…‹éš†ï¼‰
                    # 3. çº¯æ–‡æœ¬å¸–å­ï¼ˆTTSåˆæˆï¼‰
                    has_media = thread_content['has_video'] or thread_content['has_audio']
                    has_text = bool(thread_content.get('content', '').strip())

                    if has_media or has_text:
                        # åˆå¹¶ä¿¡æ¯
                        media_post = {**thread, **thread_content}
                        new_video_posts.append(media_post)

                        if has_media:
                            print(f"ğŸ¬ å‘ç°åª’ä½“å¸–å­: {thread['title']}")
                            print(f"   è§†é¢‘é“¾æ¥: {len(thread_content['video_urls'])} ä¸ª")
                            print(f"   éŸ³é¢‘é“¾æ¥: {len(thread_content['audio_urls'])} ä¸ª")
                            print(f"   é™„ä»¶: {len(thread_content['attachments'])} ä¸ª")

                            # æ˜¾ç¤ºå…·ä½“é“¾æ¥
                            for i, url in enumerate(thread_content['video_urls'], 1):
                                print(f"     è§†é¢‘{i}: {url}")
                            for i, url in enumerate(thread_content['audio_urls'], 1):
                                print(f"     éŸ³é¢‘{i}: {url}")
                        else:
                            print(f"ğŸ“ å‘ç°æ–‡æœ¬å¸–å­: {thread['title']}")
                            print(f"   å†…å®¹é•¿åº¦: {len(thread_content.get('content', ''))} å­—ç¬¦")

                        # æ˜¾ç¤ºå°é¢ä¿¡æ¯
                        if thread_content['cover_info']:
                            print(f"   å°é¢ä¿¡æ¯: {thread_content['cover_info']}")
                    else:
                        print(f"âš ï¸ å¸–å­æ— æœ‰æ•ˆå†…å®¹: {thread['title']}")

                    # æµ‹è¯•æ¨¡å¼ï¼šæ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆä»…åœ¨å†…å­˜ä¸­ï¼‰
                    self.processed_threads.add(thread_id)

            else:
                # ğŸš€ ç”Ÿäº§æ¨¡å¼ï¼šåªå¤„ç†æ–°å¸–å­ï¼ŒæŒä¹…åŒ–å»é‡
                if not self.first_check_completed:
                    # é¦–æ¬¡å¯åŠ¨ï¼šæ ‡è®°ç°æœ‰å¸–å­ä¸ºå·²å¤„ç†ï¼Œä¸å®é™…å¤„ç†
                    print("ğŸ”„ ç”Ÿäº§æ¨¡å¼é¦–æ¬¡å¯åŠ¨ï¼Œæ ‡è®°ç°æœ‰å¸–å­ä¸ºå·²å¤„ç†...")
                    for thread in threads:
                        thread_id = thread['thread_id']
                        self.mark_post_processed(thread_id)
                        print(f"ğŸ“ æ ‡è®°å·²å­˜åœ¨å¸–å­: {thread['title']} (ID: {thread_id})")

                    self.first_check_completed = True
                    print(f"âœ… é¦–æ¬¡æ£€æŸ¥å®Œæˆï¼Œå·²æ ‡è®° {len(threads)} ä¸ªç°æœ‰å¸–å­")
                    print("ğŸ” ä¸‹æ¬¡æ£€æŸ¥å°†å¤„ç†æ–°å‘å¸ƒçš„å¸–å­")
                    return []

                # æ­£å¸¸ç›‘æ§ï¼šåªå¤„ç†æ–°å¸–å­
                print("ğŸš€ ç”Ÿäº§æ¨¡å¼ï¼šåªæ£€æŸ¥æ–°å¸–å­")
                for thread in threads:
                    thread_id = thread['thread_id']

                    # è·³è¿‡å·²å¤„ç†çš„å¸–å­
                    if thread_id in self.processed_threads:
                        continue

                    print(f"ğŸ†• å‘ç°æ–°å¸–å­: {thread['title']} (ID: {thread_id})")

                    # è·å–å¸–å­è¯¦ç»†å†…å®¹
                    thread_content = self.get_thread_content(thread['thread_url'])

                    # ğŸ¯ æ”¯æŒä¸‰ç§ç±»å‹çš„å¸–å­ï¼š
                    # 1. è§†é¢‘å¸–å­ï¼ˆè§†é¢‘å¤„ç†ï¼‰
                    # 2. éŸ³é¢‘å¸–å­ï¼ˆéŸ³è‰²å…‹éš†ï¼‰
                    # 3. çº¯æ–‡æœ¬å¸–å­ï¼ˆTTSåˆæˆï¼‰
                    has_media = thread_content['has_video'] or thread_content['has_audio']
                    has_text = bool(thread_content.get('content', '').strip())

                    if has_media or has_text:
                        # åˆå¹¶ä¿¡æ¯
                        media_post = {**thread, **thread_content}
                        new_video_posts.append(media_post)

                        if has_media:
                            print(f"ğŸ¬ å‘ç°åª’ä½“å¸–å­: {thread['title']}")
                            print(f"   è§†é¢‘é“¾æ¥: {len(thread_content['video_urls'])} ä¸ª")
                            print(f"   éŸ³é¢‘é“¾æ¥: {len(thread_content['audio_urls'])} ä¸ª")
                            print(f"   é™„ä»¶: {len(thread_content['attachments'])} ä¸ª")

                            # æ˜¾ç¤ºå…·ä½“é“¾æ¥
                            for i, url in enumerate(thread_content['video_urls'], 1):
                                print(f"     è§†é¢‘{i}: {url}")
                            for i, url in enumerate(thread_content['audio_urls'], 1):
                                print(f"     éŸ³é¢‘{i}: {url}")
                        else:
                            print(f"ğŸ“ å‘ç°æ–‡æœ¬å¸–å­: {thread['title']}")
                            print(f"   å†…å®¹é•¿åº¦: {len(thread_content.get('content', ''))} å­—ç¬¦")

                        # æ˜¾ç¤ºå°é¢ä¿¡æ¯
                        if thread_content['cover_info']:
                            print(f"   å°é¢ä¿¡æ¯: {thread_content['cover_info']}")
                    else:
                        print(f"âš ï¸ æ–°å¸–å­æ— æœ‰æ•ˆå†…å®¹: {thread['title']}")

                    # ç”Ÿäº§æ¨¡å¼ï¼šæ ‡è®°ä¸ºå·²å¤„ç†å¹¶ç«‹å³ä¿å­˜
                    self.mark_post_processed(thread_id)

            if new_video_posts:
                print(f"âœ… å‘ç° {len(new_video_posts)} ä¸ªæ–°çš„è§†é¢‘å¸–å­")
            else:
                print("ğŸ“­ æš‚æ— æ–°çš„è§†é¢‘å¸–å­")

            return new_video_posts

        except Exception as e:
            print(f"âŒ ç›‘æ§æ–°å¸–å¤±è´¥: {e}")
            return []

    def get_new_posts_simple(self) -> List[Dict[str, Any]]:
        """ç®€åŒ–çš„æ–°å¸–ç›‘æ§ï¼šåªè·å–å¸–å­åˆ—è¡¨ï¼Œä¸è§£æå†…å®¹"""
        try:
            print(f"ğŸ” å¼€å§‹ç›‘æ§æ™ºèƒ½å‰ªå£æ’­æ¿å— ({datetime.now().strftime('%H:%M:%S')})")

            # è·å–æ‰€æœ‰å¸–å­ï¼ˆåªæœ‰åŸºæœ¬ä¿¡æ¯ï¼Œä¸è§£æå†…å®¹ï¼‰
            threads = self.get_forum_threads()

            new_posts = []

            # ç”Ÿäº§æ¨¡å¼ï¼šåªå¤„ç†æ–°å¸–å­ï¼ŒæŒä¹…åŒ–å»é‡
            if not self.first_check_completed:
                # é¦–æ¬¡å¯åŠ¨ï¼šæ ‡è®°ç°æœ‰å¸–å­ä¸ºå·²å¤„ç†ï¼Œä¸å®é™…å¤„ç†
                print("ğŸ”„ ç”Ÿäº§æ¨¡å¼é¦–æ¬¡å¯åŠ¨ï¼Œæ ‡è®°ç°æœ‰å¸–å­ä¸ºå·²å¤„ç†...")
                for thread in threads:
                    thread_id = thread['thread_id']
                    self.mark_post_processed(thread_id)
                    print(f"ğŸ“ æ ‡è®°å·²å­˜åœ¨å¸–å­: {thread['title']} (ID: {thread_id})")

                self.first_check_completed = True
                print(f"âœ… é¦–æ¬¡æ£€æŸ¥å®Œæˆï¼Œå·²æ ‡è®° {len(threads)} ä¸ªç°æœ‰å¸–å­")
                print("ğŸ” ä¸‹æ¬¡æ£€æŸ¥å°†å¤„ç†æ–°å‘å¸ƒçš„å¸–å­")
                return []

            # æ­£å¸¸ç›‘æ§ï¼šåªå¤„ç†æ–°å¸–å­
            print("ğŸš€ ç”Ÿäº§æ¨¡å¼ï¼šåªæ£€æŸ¥æ–°å¸–å­")
            for thread in threads:
                thread_id = thread['thread_id']

                # è·³è¿‡å·²å¤„ç†çš„å¸–å­
                if thread_id in self.processed_threads:
                    continue

                print(f"ğŸ†• å‘ç°æ–°å¸–å­: {thread['title']} (ID: {thread_id})")

                # åªè¿”å›åŸºæœ¬ä¿¡æ¯ï¼Œä¸è§£æå†…å®¹
                new_posts.append({
                    'thread_id': thread_id,
                    'title': thread['title'],
                    'thread_url': thread['thread_url'],
                    'author': thread.get('author', 'æœªçŸ¥ä½œè€…'),
                    'forum_name': 'æ™ºèƒ½å‰ªå£æ’­'
                })

                # æ ‡è®°ä¸ºå·²å¤„ç†
                self.mark_post_processed(thread_id)

            if new_posts:
                print(f"âœ… å‘ç° {len(new_posts)} ä¸ªæ–°å¸–å­")
            else:
                print("ğŸ“­ æš‚æ— æ–°å¸–å­")

            return new_posts

        except Exception as e:
            print(f"âŒ ç®€åŒ–ç›‘æ§æ–°å¸–å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return []

    def reply_to_thread(self, thread_id: str, content: str, video_files: List[str] = None) -> bool:
        """å›å¤å¸–å­ï¼Œæ”¯æŒä¸Šä¼ è§†é¢‘æ–‡ä»¶"""
        try:
            if not self.logged_in:
                print("âš ï¸ æœªç™»å½•ï¼Œæ— æ³•å›å¤å¸–å­")
                return False

            print(f"ğŸ“¤ å›å¤å¸–å­: {thread_id}")
            if video_files:
                print(f"ğŸ“ å‡†å¤‡ä¸Šä¼  {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")

            # å¦‚æœæœ‰è§†é¢‘æ–‡ä»¶ï¼Œä½¿ç”¨å®Œæ•´çš„å›å¤é¡µé¢è€Œä¸æ˜¯å¿«é€Ÿå›å¤
            if video_files:
                return self._reply_with_attachments(thread_id, content, video_files)
            else:
                return self._reply_text_only(thread_id, content)

        except Exception as e:
            print(f"âŒ å›å¤å¸–å­å¼‚å¸¸: {e}")
            return False

    def _reply_text_only(self, thread_id: str, content: str) -> bool:
        """çº¯æ–‡æœ¬å›å¤ï¼ˆå¿«é€Ÿå›å¤ï¼‰"""
        try:
            # æ„å»ºå›å¤URL
            reply_url = f"{self.base_url}/forum.php?mod=post&action=reply&tid={thread_id}&infloat=yes&handlekey=fastpost"

            # è·å–å›å¤é¡µé¢è·å–formhash
            reply_page = self.session.get(reply_url)
            soup = BeautifulSoup(reply_page.text, 'html.parser')

            form_hash = ""
            form_hash_input = soup.find('input', {'name': 'formhash'})
            if form_hash_input:
                form_hash = form_hash_input.get('value', '')

            # å›å¤æ•°æ®
            reply_data = {
                'formhash': form_hash,
                'posttime': int(time.time()),
                'message': content,
                'replysubmit': 'yes',
                'infloat': 'yes',
                'handlekey': 'fastpost',
                'inajax': '1'
            }

            # å‘é€å›å¤
            response = self.session.post(
                f"{self.base_url}/forum.php?mod=post&action=reply&tid={thread_id}&infloat=yes&handlekey=fastpost&inajax=1",
                data=reply_data
            )

            if 'å›å¤å‘å¸ƒæˆåŠŸ' in response.text or 'succeed' in response.text.lower():
                print(f"âœ… å›å¤æˆåŠŸ: {thread_id}")
                return True
            else:
                print(f"âŒ å›å¤å¤±è´¥: {thread_id}")
                print(f"å“åº”: {response.text[:200]}...")
                return False

        except Exception as e:
            print(f"âŒ æ–‡æœ¬å›å¤å¼‚å¸¸: {e}")
            return False

    def _reply_with_attachments(self, thread_id: str, content: str, video_files: List[str]) -> bool:
        """å¸¦é™„ä»¶çš„å›å¤ - æ”¯æŒè…¾è®¯äº‘ä¸Šä¼ æŒ‰é’®"""
        try:
            import os

            # æ„å»ºå®Œæ•´å›å¤é¡µé¢URL
            reply_url = f"{self.base_url}/forum.php?mod=post&action=reply&tid={thread_id}"

            # è·å–å›å¤é¡µé¢
            reply_page = self.session.get(reply_url)
            soup = BeautifulSoup(reply_page.text, 'html.parser')

            # è·å–formhash
            form_hash = ""
            form_hash_input = soup.find('input', {'name': 'formhash'})
            if form_hash_input:
                form_hash = form_hash_input.get('value', '')

            print(f"ğŸ“ è·å–åˆ°formhash: {form_hash[:10]}...")

            # æ£€æŸ¥æ˜¯å¦æœ‰è…¾è®¯äº‘ä¸Šä¼ æŒ‰é’®
            tencent_upload_button = self._find_tencent_upload_button(soup)
            if tencent_upload_button:
                print("ğŸ” å‘ç°è…¾è®¯äº‘ä¸Šä¼ æŒ‰é’®ï¼Œå°è¯•ä½¿ç”¨è…¾è®¯äº‘ä¸Šä¼ ...")
                success = self._upload_via_tencent_cloud(thread_id, content, video_files, form_hash, soup)
                if success:
                    return True
                else:
                    print("âš ï¸ è…¾è®¯äº‘ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿä¸Šä¼ æ–¹å¼...")

            # ä¼ ç»Ÿæ–‡ä»¶ä¸Šä¼ æ–¹å¼
            return self._upload_via_traditional_method(thread_id, content, video_files, form_hash)

        except Exception as e:
            print(f"âŒ å¸¦é™„ä»¶å›å¤å¼‚å¸¸: {e}")
            # å¦‚æœé™„ä»¶ä¸Šä¼ å¤±è´¥ï¼Œå°è¯•çº¯æ–‡æœ¬å›å¤
            print("ğŸ”„ å°è¯•çº¯æ–‡æœ¬å›å¤...")
            return self._reply_text_only(thread_id, content)

    def _find_tencent_upload_button(self, soup: BeautifulSoup) -> bool:
        """æŸ¥æ‰¾è…¾è®¯äº‘ä¸Šä¼ æŒ‰é’®"""
        try:
            # æŸ¥æ‰¾å¯èƒ½çš„è…¾è®¯äº‘ä¸Šä¼ æŒ‰é’®
            tencent_buttons = soup.find_all(['button', 'input', 'a'], string=re.compile(r'è…¾è®¯äº‘|ä¸Šä¼ |äº‘å­˜å‚¨', re.I))

            # æŸ¥æ‰¾åŒ…å«è…¾è®¯äº‘ç›¸å…³classæˆ–idçš„å…ƒç´ 
            tencent_elements = soup.find_all(['div', 'button', 'input'],
                                           attrs={'class': re.compile(r'tencent|cloud|upload', re.I)})
            tencent_elements.extend(soup.find_all(['div', 'button', 'input'],
                                                attrs={'id': re.compile(r'tencent|cloud|upload', re.I)}))

            if tencent_buttons or tencent_elements:
                print(f"ğŸ” å‘ç° {len(tencent_buttons)} ä¸ªè…¾è®¯äº‘æŒ‰é’®ï¼Œ{len(tencent_elements)} ä¸ªç›¸å…³å…ƒç´ ")
                return True

            return False
        except Exception as e:
            print(f"âŒ æŸ¥æ‰¾è…¾è®¯äº‘ä¸Šä¼ æŒ‰é’®å¤±è´¥: {e}")
            return False

    def _upload_via_tencent_cloud(self, thread_id: str, content: str, video_files: List[str],
                                 form_hash: str, soup: BeautifulSoup) -> bool:
        """é€šè¿‡è…¾è®¯äº‘ä¸Šä¼ æŒ‰é’®ä¸Šä¼ æ–‡ä»¶"""
        try:
            print("ğŸš€ å°è¯•è…¾è®¯äº‘ä¸Šä¼ æ–¹å¼...")

            # ç›´æ¥ä½¿ç”¨å‘ç°çš„è…¾è®¯äº‘APIç«¯ç‚¹
            tencent_api_url = f"{self.base_url}/source/plugin/tencentcos/upload_api.php"

            uploaded_files = []

            for video_file in video_files:
                if not os.path.exists(video_file):
                    print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {video_file}")
                    continue

                file_size = os.path.getsize(video_file) / (1024 * 1024)  # MB
                print(f"â˜ï¸ è…¾è®¯äº‘ä¸Šä¼ : {os.path.basename(video_file)} ({file_size:.1f} MB)")

                try:
                    with open(video_file, 'rb') as f:
                        files = {
                            'Filedata': (os.path.basename(video_file), f, 'video/mp4')
                        }

                        data = {
                            'filetype': 'video'
                        }

                        response = self.session.post(
                            tencent_api_url,
                            data=data,
                            files=files,
                            timeout=300
                        )

                        if response.status_code == 200:
                            try:
                                import json
                                response_data = json.loads(response.text)

                                if response_data.get('code') == 0 and 'data' in response_data:
                                    file_info = response_data['data']
                                    tencent_url = file_info.get('url')
                                    aid = file_info.get('aid')
                                    filename = file_info.get('filename')

                                    if tencent_url:
                                        uploaded_files.append({
                                            'url': tencent_url,
                                            'aid': aid,
                                            'filename': filename,
                                            'original_file': video_file
                                        })
                                        print(f"âœ… è…¾è®¯äº‘ä¸Šä¼ æˆåŠŸ: {filename}")
                                        print(f"ğŸ“ è…¾è®¯äº‘URL: {tencent_url}")
                                    else:
                                        print(f"âŒ è…¾è®¯äº‘å“åº”ç¼ºå°‘URL: {response.text}")
                                else:
                                    print(f"âŒ è…¾è®¯äº‘ä¸Šä¼ å¤±è´¥: {response_data.get('message', 'æœªçŸ¥é”™è¯¯')}")

                            except json.JSONDecodeError:
                                print(f"âŒ è…¾è®¯äº‘å“åº”è§£æå¤±è´¥: {response.text[:200]}")
                        else:
                            print(f"âŒ è…¾è®¯äº‘ä¸Šä¼ HTTPé”™è¯¯: {response.status_code}")

                except Exception as e:
                    print(f"âŒ è…¾è®¯äº‘ä¸Šä¼ å¼‚å¸¸: {e}")

            if uploaded_files:
                print(f"ğŸ‰ è…¾è®¯äº‘ä¸Šä¼ å®Œæˆï¼ŒæˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶")
                # å‘é€åŒ…å«è…¾è®¯äº‘é“¾æ¥çš„å›å¤
                return self._send_reply_with_tencent_links(thread_id, content, uploaded_files, form_hash)
            else:
                print("âŒ æ²¡æœ‰æ–‡ä»¶æˆåŠŸä¸Šä¼ åˆ°è…¾è®¯äº‘")
                return False

        except Exception as e:
            print(f"âŒ è…¾è®¯äº‘ä¸Šä¼ å¼‚å¸¸: {e}")
            return False

    def _send_reply_with_tencent_links(self, thread_id: str, content: str,
                                     uploaded_files: List[Dict], form_hash: str) -> bool:
        """å‘é€åŒ…å«è…¾è®¯äº‘é“¾æ¥çš„å›å¤"""
        try:
            print("ğŸ“ å‘é€åŒ…å«è…¾è®¯äº‘BBCodeé“¾æ¥çš„å›å¤...")

            # æ„å»ºåŒ…å«è…¾è®¯äº‘æ–‡ä»¶ä¿¡æ¯çš„å›å¤å†…å®¹
            enhanced_content = content + "\n\nğŸ¬ è§†é¢‘æ–‡ä»¶å·²é€šè¿‡è…¾è®¯äº‘ä¸Šä¼ æˆåŠŸï¼\n\nğŸ“ ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨ï¼š"

            for i, file_info in enumerate(uploaded_files, 1):
                filename = file_info['filename']
                tencent_url = file_info['url']
                aid = file_info.get('aid', '')

                # ä½¿ç”¨BBCodeæ ¼å¼çš„URLæ ‡ç­¾
                bbcode_link = f"[url={tencent_url}][color=#2B7ACD][b]{filename}[/b][/color][/url]"

                enhanced_content += f"\n{i}. {bbcode_link}"
                if aid:
                    enhanced_content += f" (é™„ä»¶ID: {aid})"
                enhanced_content += "\n"

            enhanced_content += "\nğŸš€ ä¸Šä¼ æ–¹å¼: è…¾è®¯äº‘COS"
            enhanced_content += "\nâš¡ æ”¯æŒé«˜é€Ÿä¸‹è½½å’Œåœ¨çº¿æ’­æ”¾"
            enhanced_content += "\nğŸ”— ç‚¹å‡»æ–‡ä»¶åå³å¯ä¸‹è½½æˆ–æ’­æ”¾"
            enhanced_content += f"\nğŸ•’ ä¸Šä¼ æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

            # å‘é€å›å¤
            reply_data = {
                'formhash': form_hash,
                'posttime': int(time.time()),
                'message': enhanced_content,
                'replysubmit': 'yes',
                'wysiwyg': '0',
                'checkbox': '0'
            }

            response = self.session.post(
                f"{self.base_url}/forum.php?mod=post&action=reply&tid={thread_id}",
                data=reply_data,
                timeout=60
            )

            # æ£€æŸ¥å›å¤ç»“æœ
            if 'å‘å¸ƒæˆåŠŸ' in response.text or 'å›å¤å‘å¸ƒæˆåŠŸ' in response.text or 'succeed' in response.text.lower():
                print(f"âœ… è…¾è®¯äº‘BBCodeé“¾æ¥å›å¤æˆåŠŸ: {thread_id}")
                print(f"ğŸ“ åŒ…å« {len(uploaded_files)} ä¸ªè…¾è®¯äº‘BBCodeé“¾æ¥")

                # æ˜¾ç¤ºç”Ÿæˆçš„BBCodeé“¾æ¥
                print("ğŸ”— ç”Ÿæˆçš„BBCodeé“¾æ¥:")
                for i, file_info in enumerate(uploaded_files, 1):
                    filename = file_info['filename']
                    tencent_url = file_info['url']
                    bbcode_link = f"[url={tencent_url}][color=#2B7ACD][b]{filename}[/b][/color][/url]"
                    print(f"  {i}. {bbcode_link}")

                return True
            else:
                print(f"âŒ è…¾è®¯äº‘BBCodeé“¾æ¥å›å¤å¤±è´¥: {thread_id}")
                print(f"å“åº”çŠ¶æ€: {response.status_code}")
                return False

        except Exception as e:
            print(f"âŒ è…¾è®¯äº‘BBCodeé“¾æ¥å›å¤å¼‚å¸¸: {e}")
            return False

    def _upload_via_traditional_method(self, thread_id: str, content: str, video_files: List[str],
                                     form_hash: str) -> bool:
        """ä¼ ç»Ÿæ–‡ä»¶ä¸Šä¼ æ–¹å¼"""
        try:
            import os

            # å‡†å¤‡æ–‡ä»¶ä¸Šä¼ 
            files = {}

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¹¶å‡†å¤‡ä¸Šä¼ 
            valid_files = []
            for i, video_file in enumerate(video_files):
                if os.path.exists(video_file):
                    file_size = os.path.getsize(video_file) / (1024 * 1024)  # MB
                    print(f"ğŸ“ å‡†å¤‡ä¸Šä¼ æ–‡ä»¶ {i+1}: {os.path.basename(video_file)} ({file_size:.1f} MB)")

                    # ä¸é™åˆ¶æ–‡ä»¶å¤§å°ï¼Œç›´æ¥æ·»åŠ åˆ°ä¸Šä¼ åˆ—è¡¨
                    valid_files.append(video_file)

                    # å‡†å¤‡æ–‡ä»¶ä¸Šä¼ æ•°æ®
                    file_key = f'attach_{i+1}'
                    files[file_key] = (
                        os.path.basename(video_file),
                        open(video_file, 'rb'),
                        'video/mp4'
                    )
                else:
                    print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {video_file}")

            if not valid_files:
                print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶å¯ä¸Šä¼ ï¼Œä½¿ç”¨çº¯æ–‡æœ¬å›å¤")
                return self._reply_text_only(thread_id, content)

            # æ„å»ºå›å¤æ•°æ®
            reply_data = {
                'formhash': form_hash,
                'posttime': int(time.time()),
                'message': content,
                'replysubmit': 'yes',
                'wysiwyg': '0',
                'checkbox': '0'
            }

            print(f"ğŸ“¤ å¼€å§‹ä¼ ç»Ÿæ–¹å¼ä¸Šä¼ å›å¤ï¼ˆåŒ…å« {len(valid_files)} ä¸ªæ–‡ä»¶ï¼‰...")

            # å‘é€å¸¦é™„ä»¶çš„å›å¤
            response = self.session.post(
                f"{self.base_url}/forum.php?mod=post&action=reply&tid={thread_id}",
                data=reply_data,
                files=files,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶ï¼Œå› ä¸ºæ–‡ä»¶ä¸Šä¼ å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
            )

            # å…³é—­æ–‡ä»¶å¥æŸ„
            for file_obj in files.values():
                if hasattr(file_obj[1], 'close'):
                    file_obj[1].close()

            # æ£€æŸ¥å›å¤ç»“æœ
            if 'å‘å¸ƒæˆåŠŸ' in response.text or 'å›å¤å‘å¸ƒæˆåŠŸ' in response.text or 'succeed' in response.text.lower():
                print(f"âœ… ä¼ ç»Ÿæ–¹å¼ä¸Šä¼ æˆåŠŸ: {thread_id}")
                print(f"ğŸ“ æˆåŠŸä¸Šä¼  {len(valid_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
                return True
            else:
                print(f"âŒ ä¼ ç»Ÿæ–¹å¼ä¸Šä¼ å¤±è´¥: {thread_id}")
                print(f"å“åº”çŠ¶æ€: {response.status_code}")
                print(f"å“åº”å†…å®¹: {response.text[:500]}...")
                return False

        except Exception as e:
            print(f"âŒ ä¼ ç»Ÿä¸Šä¼ æ–¹å¼å¼‚å¸¸: {e}")
            return False


def test_crawler():
    """æµ‹è¯•çˆ¬è™«åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ™ºèƒ½å‰ªå£æ’­æ¿å—çˆ¬è™«")
    print("=" * 50)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = AicutForumCrawler()
    
    # æµ‹è¯•è·å–å¸–å­åˆ—è¡¨
    threads = crawler.get_forum_threads()
    print(f"ğŸ“Š è·å–åˆ° {len(threads)} ä¸ªå¸–å­")
    
    # æµ‹è¯•è·å–å¸–å­å†…å®¹
    if threads:
        first_thread = threads[0]
        print(f"\nğŸ“– æµ‹è¯•è·å–å¸–å­å†…å®¹: {first_thread['title']}")
        content = crawler.get_thread_content(first_thread['thread_url'])
        print(f"å†…å®¹é•¿åº¦: {len(content['content'])}")
        print(f"è§†é¢‘é“¾æ¥: {content['video_urls']}")
        print(f"é™„ä»¶: {len(content['attachments'])}")
    
    # æµ‹è¯•ç›‘æ§åŠŸèƒ½
    print(f"\nğŸ” æµ‹è¯•ç›‘æ§åŠŸèƒ½...")
    new_posts = crawler.monitor_new_posts()
    print(f"å‘ç° {len(new_posts)} ä¸ªæ–°çš„è§†é¢‘å¸–å­")


if __name__ == "__main__":
    test_crawler()
