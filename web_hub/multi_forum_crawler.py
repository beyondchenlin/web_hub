#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šè®ºå›çˆ¬è™«ç®¡ç†å™¨
æ”¯æŒåŒæ—¶ç›‘æ§å¤šä¸ªè®ºå›ç½‘ç«™

åŠŸèƒ½ï¼š
1. ä»ç¯å¢ƒå˜é‡è¯»å–å¤šä¸ªè®ºå›é…ç½®
2. å¹¶è¡Œç›‘æ§å¤šä¸ªè®ºå›
3. ç»Ÿä¸€çš„ä»»åŠ¡æäº¤æ¥å£
4. é…ç½®éªŒè¯å’ŒçŠ¶æ€ç›‘æ§

ä½¿ç”¨æ–¹æ³•ï¼š
from multi_forum_crawler import MultiForumCrawler
crawler = MultiForumCrawler()
crawler.start_monitoring()
"""

import os
import time
import threading
from typing import Dict, List, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from forum_config_manager import ForumConfigManager, ForumConfig
from aicut_forum_crawler import AicutForumCrawler

class MultiForumCrawler:
    """å¤šè®ºå›çˆ¬è™«ç®¡ç†å™¨"""
    
    def __init__(self, config_file: str = ".env"):
        self.config_manager = ForumConfigManager(config_file)
        # ä»é…ç½®æ–‡ä»¶åŠ è½½æ‰€æœ‰è®ºå›
        self.config_manager.load_all_forums_from_settings()
        self.crawlers: Dict[str, AicutForumCrawler] = {}
        self.running = False
        self.threads: Dict[str, threading.Thread] = {}
        self.stop_event = threading.Event()

        # åˆå§‹åŒ–çˆ¬è™«å®ä¾‹
        self._initialize_crawlers()
    
    def _initialize_crawlers(self):
        """åˆå§‹åŒ–æ‰€æœ‰è®ºå›çˆ¬è™«"""
        forum_configs = self.config_manager.get_enabled_forum_configs()
        
        if not forum_configs:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„è®ºå›é…ç½®")
            return
        
        print(f"ğŸš€ åˆå§‹åŒ– {len(forum_configs)} ä¸ªè®ºå›çˆ¬è™«...")
        
        for name, config in forum_configs.items():
            try:
                crawler = AicutForumCrawler(
                    username=config.username,
                    password=config.password,
                    test_mode=config.test_mode,
                    test_once=config.test_once,
                    base_url=config.base_url,
                    forum_url=config.target_url
                )
                
                # å°è¯•ç™»å½•
                if crawler.login():
                    self.crawlers[name] = crawler
                    print(f"âœ… è®ºå› {config.name} åˆå§‹åŒ–æˆåŠŸ")
                else:
                    print(f"âŒ è®ºå› {config.name} ç™»å½•å¤±è´¥")
                    
            except Exception as e:
                print(f"âŒ è®ºå› {config.name} åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§æ‰€æœ‰è®ºå›"""
        if not self.crawlers:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„è®ºå›çˆ¬è™«")
            return False
        
        print(f"ğŸ” å¼€å§‹ç›‘æ§ {len(self.crawlers)} ä¸ªè®ºå›...")
        self.running = True
        self.stop_event.clear()
        
        # ä¸ºæ¯ä¸ªè®ºå›åˆ›å»ºç›‘æ§çº¿ç¨‹
        for name, crawler in self.crawlers.items():
            config = self.config_manager.get_forum_config(name)
            thread = threading.Thread(
                target=self._monitor_forum,
                args=(name, crawler, config),
                daemon=True
            )
            thread.start()
            self.threads[name] = thread
            print(f"ğŸ¯ è®ºå› {config.name} ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
        
        return True
    
    def _monitor_forum(self, name: str, crawler: AicutForumCrawler, config: ForumConfig):
        """ç›‘æ§å•ä¸ªè®ºå›"""
        print(f"ğŸ“¡ å¼€å§‹ç›‘æ§è®ºå›: {config.name}")
        
        while self.running and not self.stop_event.is_set():
            try:
                # è·å–è®ºå›å¸–å­
                threads = crawler.get_forum_threads()
                
                if threads:
                    print(f"ğŸ“‹ è®ºå› {config.name} å‘ç° {len(threads)} ä¸ªå¸–å­")
                    
                    # å¤„ç†æ¯ä¸ªå¸–å­
                    for thread_info in threads:
                        if self.stop_event.is_set():
                            break
                        
                        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                        thread_id = thread_info['thread_id']
                        if thread_id in crawler.processed_threads:
                            continue
                        
                        # è·å–å¸–å­è¯¦ç»†å†…å®¹
                        content_info = crawler.get_thread_content(thread_info['thread_url'])
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘å†…å®¹
                        if content_info.get('has_video', False):
                            # æäº¤å¤„ç†ä»»åŠ¡
                            self._submit_processing_task(name, thread_info, content_info)
                        
                        # æ ‡è®°ä¸ºå·²å¤„ç†
                        crawler.mark_post_processed(thread_id)
                
                # å¦‚æœæ˜¯å•æ¬¡è¿è¡Œæ¨¡å¼ï¼Œé€€å‡ºå¾ªç¯
                if config.test_once:
                    print(f"ğŸ§ª è®ºå› {config.name} å•æ¬¡è¿è¡Œå®Œæˆ")
                    break
                
                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                if not self.stop_event.wait(config.check_interval):
                    continue
                else:
                    break
                    
            except Exception as e:
                print(f"âŒ ç›‘æ§è®ºå› {config.name} æ—¶å‡ºé”™: {e}")
                # å‡ºé”™æ—¶ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
                if not self.stop_event.wait(60):
                    continue
                else:
                    break
        
        print(f"ğŸ›‘ è®ºå› {config.name} ç›‘æ§å·²åœæ­¢")
    
    def _submit_processing_task(self, forum_name: str, thread_info: Dict[str, Any], content_info: Dict[str, Any]):
        """æäº¤è§†é¢‘å¤„ç†ä»»åŠ¡"""
        try:
            # å¯¼å…¥ä»»åŠ¡æäº¤æ¨¡å—
            from lightweight.queue_manager import QueueManager
            
            # åˆ›å»ºä»»åŠ¡æ•°æ®
            task_data = {
                'forum_name': forum_name,
                'thread_id': thread_info['thread_id'],
                'title': thread_info['title'],
                'author': thread_info['author'],
                'thread_url': thread_info['thread_url'],
                'video_urls': content_info['video_urls'],
                'original_filenames': content_info.get('original_filenames', []),
                'audio_urls': content_info.get('audio_urls', []),
                'content': content_info['content'],
                'cover_info': content_info.get('cover_info', {}),
                'forum_id': thread_info.get('forum_id', 1),
                'forum_name_display': thread_info.get('forum_name', 'æœªçŸ¥æ¿å—')
            }
            
            # æäº¤åˆ°é˜Ÿåˆ—
            queue_manager = QueueManager()
            task_id = queue_manager.submit_task(task_data)
            
            print(f"âœ… å·²æäº¤å¤„ç†ä»»åŠ¡: {thread_info['title']} (ä»»åŠ¡ID: {task_id})")
            
        except Exception as e:
            print(f"âŒ æäº¤å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§æ‰€æœ‰è®ºå›"""
        print("ğŸ›‘ åœæ­¢è®ºå›ç›‘æ§...")
        self.running = False
        self.stop_event.set()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ç»“æŸ
        for name, thread in self.threads.items():
            if thread.is_alive():
                print(f"â³ ç­‰å¾…è®ºå› {name} ç›‘æ§çº¿ç¨‹ç»“æŸ...")
                thread.join(timeout=10)
        
        self.threads.clear()
        print("âœ… æ‰€æœ‰è®ºå›ç›‘æ§å·²åœæ­¢")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§çŠ¶æ€"""
        status = {
            'running': self.running,
            'total_forums': len(self.crawlers),
            'active_threads': len([t for t in self.threads.values() if t.is_alive()]),
            'forums': {}
        }
        
        for name, crawler in self.crawlers.items():
            config = self.config_manager.get_forum_config(name)
            thread = self.threads.get(name)
            
            status['forums'][name] = {
                'name': config.name if config else name,
                'base_url': config.base_url if config else 'Unknown',
                'enabled': config.enabled if config else False,
                'thread_alive': thread.is_alive() if thread else False,
                'processed_posts': len(crawler.processed_threads),
                'test_mode': config.test_mode if config else False
            }
        
        return status
    
    def print_status(self):
        """æ‰“å°ç›‘æ§çŠ¶æ€"""
        status = self.get_status()
        
        print("\nğŸ“Š å¤šè®ºå›ç›‘æ§çŠ¶æ€")
        print("=" * 50)
        print(f"è¿è¡ŒçŠ¶æ€: {'ğŸŸ¢ è¿è¡Œä¸­' if status['running'] else 'ğŸ”´ å·²åœæ­¢'}")
        print(f"è®ºå›æ€»æ•°: {status['total_forums']}")
        print(f"æ´»è·ƒçº¿ç¨‹: {status['active_threads']}")
        print()
        
        for name, forum_status in status['forums'].items():
            thread_status = "ğŸŸ¢ æ´»è·ƒ" if forum_status['thread_alive'] else "ğŸ”´ åœæ­¢"
            mode = "ğŸ§ª æµ‹è¯•" if forum_status['test_mode'] else "ğŸš€ ç”Ÿäº§"
            
            print(f"ğŸ“ {forum_status['name']} ({name})")
            print(f"   çŠ¶æ€: {thread_status}")
            print(f"   æ¨¡å¼: {mode}")
            print(f"   ç½‘ç«™: {forum_status['base_url']}")
            print(f"   å·²å¤„ç†: {forum_status['processed_posts']} ä¸ªå¸–å­")
            print()


def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•å’Œç‹¬ç«‹è¿è¡Œ"""
    import argparse
    import signal
    
    parser = argparse.ArgumentParser(description="å¤šè®ºå›çˆ¬è™«ç®¡ç†å™¨")
    parser.add_argument("--config", default=".env", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--status", action="store_true", help="æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯")
    parser.add_argument("--test-once", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼ˆå•æ¬¡è¿è¡Œï¼‰")
    
    args = parser.parse_args()
    
    # åˆ›å»ºçˆ¬è™«ç®¡ç†å™¨
    crawler_manager = MultiForumCrawler(args.config)
    
    if args.status:
        crawler_manager.print_status()
        return
    
    # è®¾ç½®ä¿¡å·å¤„ç†
    def signal_handler(signum, frame):
        print("\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
        crawler_manager.stop_monitoring()
        exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # éªŒè¯é…ç½®
    is_valid, errors = crawler_manager.config_manager.validate_configs()
    if not is_valid:
        print("âŒ é…ç½®éªŒè¯å¤±è´¥:")
        for error in errors:
            print(f"   - {error}")
        return
    
    # æ˜¾ç¤ºé…ç½®æ‘˜è¦
    crawler_manager.config_manager.print_config_summary()
    
    # å¼€å§‹ç›‘æ§
    if crawler_manager.start_monitoring():
        print("\nğŸ¯ å¤šè®ºå›ç›‘æ§å·²å¯åŠ¨")
        print("æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
        
        try:
            # ä¸»å¾ªç¯ - å®šæœŸæ˜¾ç¤ºçŠ¶æ€
            while crawler_manager.running:
                time.sleep(60)  # æ¯åˆ†é’Ÿæ˜¾ç¤ºä¸€æ¬¡çŠ¶æ€
                if not args.test_once:
                    crawler_manager.print_status()
        except KeyboardInterrupt:
            pass
        finally:
            crawler_manager.stop_monitoring()
    else:
        print("âŒ å¯åŠ¨å¤šè®ºå›ç›‘æ§å¤±è´¥")


if __name__ == "__main__":
    main()
