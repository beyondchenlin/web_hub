#!/usr/bin/env python3
"""
æ··åˆæ•°æ®ç®¡ç†å™¨ - SQLite + Redis æ··åˆå­˜å‚¨ç®¡ç†
æ”¯æŒè®ºå›å¸–å­æ•°æ®çš„æŒä¹…åŒ–å­˜å‚¨å’Œé«˜é€Ÿç¼“å­˜

ä¸»è¦åŠŸèƒ½:
1. SQLiteæŒä¹…åŒ–å­˜å‚¨ - æ•°æ®å®‰å…¨ä¿éšœ
2. Redisé«˜é€Ÿç¼“å­˜ - æå‡è®¿é—®æ€§èƒ½
3. è‡ªåŠ¨æ•°æ®åŒæ­¥ - ä¿è¯æ•°æ®ä¸€è‡´æ€§
4. æ•…éšœæ¢å¤æœºåˆ¶ - Redisä¸å¯ç”¨æ—¶é™çº§åˆ°SQLite
"""

import os
import sys
import json
import sqlite3
import logging
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from contextlib import contextmanager

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥ç‰ˆæœ¬ç®¡ç†é…ç½®
from database_version import get_current_version, get_required_columns

try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    print("âš ï¸ Redisä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨çº¯SQLiteæ¨¡å¼")


@dataclass
class ForumPost:
    """è®ºå›å¸–å­æ•°æ®æ¨¡å‹"""
    post_id: str
    thread_id: str
    forum_id: int = 2
    title: str = ""
    content: str = ""
    author_id: str = ""
    author_name: str = ""
    cover_title_up: str = ""
    cover_title_middle: str = ""
    cover_title_down: str = ""
    cover_info_raw: str = ""
    video_urls: List[str] = None
    audio_urls: List[str] = None
    original_filenames: List[str] = None  # æ–°å¢ï¼šå­˜å‚¨åŸå§‹æ–‡ä»¶å
    media_count: int = 0
    processing_status: str = "pending"
    task_id: str = ""
    output_path: str = ""
    reply_status: str = "pending"
    reply_content: str = ""
    reply_time: Optional[datetime] = None
    post_time: Optional[datetime] = None
    discovered_time: Optional[datetime] = None
    last_updated: Optional[datetime] = None
    metadata: Dict[str, Any] = None
    source_url: str = ""
    is_processed: bool = False
    is_replied: bool = False
    priority: int = 1

    def __post_init__(self):
        if self.video_urls is None:
            self.video_urls = []
        if self.audio_urls is None:
            self.audio_urls = []
        if self.original_filenames is None:
            self.original_filenames = []
        if self.metadata is None:
            self.metadata = {}
        if self.discovered_time is None:
            self.discovered_time = datetime.now()
        if self.last_updated is None:
            self.last_updated = datetime.now()

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        data = asdict(self)
        # å¤„ç†datetimeå¯¹è±¡
        for key, value in data.items():
            if isinstance(value, datetime):
                data[key] = value.isoformat()
        # å¤„ç†åˆ—è¡¨å’Œå­—å…¸
        data['video_urls'] = json.dumps(self.video_urls)
        data['audio_urls'] = json.dumps(self.audio_urls)
        data['original_filenames'] = json.dumps(self.original_filenames or [])
        data['metadata'] = json.dumps(self.metadata)
        return data

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ForumPost':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        # åˆ›å»ºæ•°æ®å‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        data = dict(data)

        # ç¡®ä¿å°é¢æ ‡é¢˜å­—æ®µå­˜åœ¨ä¸”ä¸ä¸ºNone
        if 'cover_title_up' not in data:
            data['cover_title_up'] = ''
        if 'cover_title_middle' not in data:
            data['cover_title_middle'] = ''
        if 'cover_title_down' not in data:
            data['cover_title_down'] = ''

        # å¤„ç†datetimeå­—æ®µ
        datetime_fields = ['reply_time', 'post_time', 'discovered_time', 'last_updated']
        for field in datetime_fields:
            if data.get(field) and isinstance(data[field], str):
                try:
                    data[field] = datetime.fromisoformat(data[field])
                except ValueError:
                    data[field] = None

        # å¤„ç†JSONå­—æ®µ
        if isinstance(data.get('video_urls'), str):
            data['video_urls'] = json.loads(data['video_urls'])
        if isinstance(data.get('audio_urls'), str):
            data['audio_urls'] = json.loads(data['audio_urls'])
        if isinstance(data.get('original_filenames'), str):
            data['original_filenames'] = json.loads(data['original_filenames'])
        if isinstance(data.get('metadata'), str):
            data['metadata'] = json.loads(data['metadata'])

        return cls(**data)


class HybridForumDataManager:
    """æ··åˆè®ºå›æ•°æ®ç®¡ç†å™¨"""

    # æ•°æ®åº“ç‰ˆæœ¬ - ä»ç»Ÿä¸€é…ç½®è·å–
    DATABASE_VERSION = get_current_version()

    def __init__(self, db_path: str = "data/forum_posts.db",
                 redis_host: str = "localhost", redis_port: int = 6379, redis_db: int = 1):
        self.db_path = db_path
        self.redis_config = {
            'host': redis_host,
            'port': redis_port,
            'db': redis_db,
            'decode_responses': True
        }
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        # çº¿ç¨‹é”
        self._lock = threading.RLock()
        
        # åˆå§‹åŒ–å­˜å‚¨
        self._init_sqlite()
        self._init_redis()
        
        print(f"ğŸ“Š æ··åˆæ•°æ®ç®¡ç†å™¨å·²åˆå§‹åŒ–")
        print(f"   SQLite: {self.db_path}")
        print(f"   Redis: {'å¯ç”¨' if self.redis_client else 'ä¸å¯ç”¨'}")

    def _init_sqlite(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“ - å·¥ä¸šçº§ç®€æ´æ–¹æ¡ˆ"""
        try:
            # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(self.db_path), exist_ok=True)

            # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘å…³é”®å­—æ®µ
            if self._missing_required_columns():
                self.logger.info("æ£€æµ‹åˆ°æ•°æ®åº“ç»“æ„ä¸å®Œæ•´ï¼Œé‡æ–°åˆå§‹åŒ–")
                # ç®€å•ç²—æš´ï¼šåˆ é™¤æ—§æ•°æ®åº“ï¼Œé‡æ–°åˆ›å»º
                if os.path.exists(self.db_path):
                    backup_path = f"{self.db_path}.backup"
                    os.rename(self.db_path, backup_path)
                    self.logger.info(f"å·²å¤‡ä»½æ—§æ•°æ®åº“åˆ°: {backup_path}")

            # æ‰§è¡ŒSQLè„šæœ¬åˆ›å»ºè¡¨ç»“æ„
            sql_script_path = "forum_posts.sql"
            if os.path.exists(sql_script_path):
                with open(sql_script_path, 'r', encoding='utf-8') as f:
                    sql_script = f.read()

                with self._get_db_connection() as conn:
                    conn.executescript(sql_script)
                    conn.commit()

                self.logger.info("SQLiteæ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.logger.warning(f"SQLè„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: {sql_script_path}")

        except Exception as e:
            self.logger.error(f"SQLiteåˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _missing_required_columns(self):
        """æ£€æŸ¥æ˜¯å¦ç¼ºå°‘å¿…éœ€çš„åˆ—æˆ–ç‰ˆæœ¬ä¸åŒ¹é…"""
        if not os.path.exists(self.db_path):
            return False

        try:
            with self._get_db_connection() as conn:
                # æ£€æŸ¥å…³é”®å­—æ®µ
                cursor = conn.execute('PRAGMA table_info(forum_posts)')
                columns = cursor.fetchall()
                column_names = [col[1] for col in columns]

                if 'cover_title_middle' not in column_names:
                    self.logger.info("ç¼ºå°‘cover_title_middleå­—æ®µ")
                    return True

                # æ£€æŸ¥ç‰ˆæœ¬
                try:
                    cursor = conn.execute(
                        "SELECT config_value FROM system_config WHERE config_key = 'database_version'"
                    )
                    result = cursor.fetchone()
                    current_version = result[0] if result else "1.0"

                    if current_version != self.DATABASE_VERSION:
                        self.logger.info(f"æ•°æ®åº“ç‰ˆæœ¬ä¸åŒ¹é…: {current_version} != {self.DATABASE_VERSION}")
                        return True

                except:
                    # system_configè¡¨ä¸å­˜åœ¨æˆ–æŸ¥è¯¢å¤±è´¥
                    self.logger.info("æ— æ³•æ£€æŸ¥æ•°æ®åº“ç‰ˆæœ¬ï¼Œéœ€è¦é‡å»º")
                    return True

                return False

        except:
            return True  # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œå‡è®¾éœ€è¦é‡å»º



    def _init_redis(self):
        """åˆå§‹åŒ–Redisè¿æ¥"""
        self.redis_client = None
        if not REDIS_AVAILABLE:
            return
        
        try:
            self.redis_client = redis.Redis(**self.redis_config)
            # æµ‹è¯•è¿æ¥
            self.redis_client.ping()
            self.logger.info("Redisè¿æ¥æˆåŠŸ")
        except Exception as e:
            self.logger.warning(f"Redisè¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨çº¯SQLiteæ¨¡å¼: {e}")
            self.redis_client = None

    @contextmanager
    def _get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row  # ä½¿ç»“æœå¯ä»¥æŒ‰åˆ—åè®¿é—®
        try:
            yield conn
        finally:
            conn.close()

    def _get_cache_key(self, post_id: str) -> str:
        """ç”ŸæˆRedisç¼“å­˜é”®"""
        return f"forum_post:{post_id}"

    def save_post(self, post: ForumPost) -> bool:
        """ä¿å­˜è®ºå›å¸–å­"""
        try:
            with self._lock:
                # ä¿å­˜åˆ°SQLite
                success = self._save_to_sqlite(post)
                if success:
                    # æ›´æ–°Redisç¼“å­˜
                    self._update_cache(post)
                    self.logger.info(f"å¸–å­ä¿å­˜æˆåŠŸ: {post.post_id}")
                    return True
                return False
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¸–å­å¤±è´¥: {e}")
            return False

    def _save_to_sqlite(self, post: ForumPost) -> bool:
        """ä¿å­˜åˆ°SQLiteæ•°æ®åº“"""
        try:
            post_data = post.to_dict()
            
            # æ„å»ºSQLè¯­å¥
            columns = list(post_data.keys())
            placeholders = ['?' for _ in columns]
            values = list(post_data.values())
            
            sql = f"""
            INSERT OR REPLACE INTO forum_posts ({', '.join(columns)})
            VALUES ({', '.join(placeholders)})
            """
            
            with self._get_db_connection() as conn:
                conn.execute(sql, values)
                conn.commit()
            
            return True
        except Exception as e:
            self.logger.error(f"SQLiteä¿å­˜å¤±è´¥: {e}")
            return False

    def _update_cache(self, post: ForumPost):
        """æ›´æ–°Redisç¼“å­˜"""
        if not self.redis_client:
            return
        
        try:
            cache_key = self._get_cache_key(post.post_id)
            post_json = json.dumps(post.to_dict(), ensure_ascii=False)
            
            # è®¾ç½®ç¼“å­˜ï¼Œè¿‡æœŸæ—¶é—´1å°æ—¶
            self.redis_client.setex(cache_key, 3600, post_json)
            
            # æ›´æ–°ç´¢å¼•
            self._update_cache_indexes(post)
            
        except Exception as e:
            self.logger.warning(f"Redisç¼“å­˜æ›´æ–°å¤±è´¥: {e}")

    def _update_cache_indexes(self, post: ForumPost):
        """æ›´æ–°Redisç´¢å¼•"""
        if not self.redis_client:
            return
        
        try:
            # æŒ‰çŠ¶æ€ç´¢å¼•
            status_key = f"posts_by_status:{post.processing_status}"
            self.redis_client.sadd(status_key, post.post_id)
            self.redis_client.expire(status_key, 3600)
            
            # æŒ‰ä¼˜å…ˆçº§ç´¢å¼•
            priority_key = f"posts_by_priority:{post.priority}"
            self.redis_client.sadd(priority_key, post.post_id)
            self.redis_client.expire(priority_key, 3600)
            
        except Exception as e:
            self.logger.warning(f"Redisç´¢å¼•æ›´æ–°å¤±è´¥: {e}")

    def get_post(self, post_id: str) -> Optional[ForumPost]:
        """è·å–è®ºå›å¸–å­"""
        try:
            # å…ˆå°è¯•ä»Redisç¼“å­˜è·å–
            if self.redis_client:
                cached_data = self._get_from_cache(post_id)
                if cached_data:
                    return cached_data

            # ä»SQLiteè·å–
            return self._get_from_sqlite(post_id)

        except Exception as e:
            self.logger.error(f"è·å–å¸–å­å¤±è´¥: {e}")
            return None

    def _get_from_cache(self, post_id: str) -> Optional[ForumPost]:
        """ä»Redisç¼“å­˜è·å–"""
        try:
            cache_key = self._get_cache_key(post_id)
            cached_json = self.redis_client.get(cache_key)
            if cached_json:
                post_data = json.loads(cached_json)
                return ForumPost.from_dict(post_data)
        except Exception as e:
            self.logger.warning(f"Redisç¼“å­˜è¯»å–å¤±è´¥: {e}")
        return None

    def _get_from_sqlite(self, post_id: str) -> Optional[ForumPost]:
        """ä»SQLiteè·å–"""
        try:
            with self._get_db_connection() as conn:
                cursor = conn.execute(
                    "SELECT * FROM forum_posts WHERE post_id = ?",
                    (post_id,)
                )
                row = cursor.fetchone()
                if row:
                    post_data = dict(row)
                    # ç§»é™¤æ•°æ®åº“çš„idå­—æ®µï¼Œå› ä¸ºForumPostç±»ä¸éœ€è¦å®ƒ
                    if 'id' in post_data:
                        del post_data['id']
                    post = ForumPost.from_dict(post_data)
                    # æ›´æ–°ç¼“å­˜
                    self._update_cache(post)
                    return post
        except Exception as e:
            self.logger.error(f"SQLiteæŸ¥è¯¢å¤±è´¥: {e}")
        return None

    def get_posts_by_status(self, status: str, limit: int = 100) -> List[ForumPost]:
        """æŒ‰çŠ¶æ€è·å–å¸–å­åˆ—è¡¨"""
        try:
            with self._get_db_connection() as conn:
                cursor = conn.execute(
                    """SELECT * FROM forum_posts
                       WHERE processing_status = ?
                       ORDER BY priority DESC, discovered_time ASC
                       LIMIT ?""",
                    (status, limit)
                )
                posts = []
                for row in cursor.fetchall():
                    post_data = dict(row)
                    # ç§»é™¤æ•°æ®åº“çš„idå­—æ®µï¼Œå› ä¸ºForumPostç±»ä¸éœ€è¦å®ƒ
                    if 'id' in post_data:
                        del post_data['id']
                    posts.append(ForumPost.from_dict(post_data))
                return posts
        except Exception as e:
            self.logger.error(f"æŒ‰çŠ¶æ€æŸ¥è¯¢å¸–å­å¤±è´¥: {e}")
            return []

    def get_pending_posts(self, limit: int = 50) -> List[ForumPost]:
        """è·å–å¾…å¤„ç†çš„å¸–å­"""
        return self.get_posts_by_status("pending", limit)

    def update_post_status(self, post_id: str, status: str, **kwargs) -> bool:
        """æ›´æ–°å¸–å­çŠ¶æ€"""
        try:
            with self._lock:
                # æ„å»ºæ›´æ–°å­—æ®µ
                update_fields = ["processing_status = ?"]
                values = [status]

                # æ·»åŠ å…¶ä»–æ›´æ–°å­—æ®µ
                for key, value in kwargs.items():
                    if key in ['task_id', 'output_path', 'reply_status', 'reply_content']:
                        update_fields.append(f"{key} = ?")
                        values.append(value)

                # æ·»åŠ æ›´æ–°æ—¶é—´
                update_fields.append("last_updated = ?")
                values.append(datetime.now().isoformat())
                values.append(post_id)

                sql = f"""
                UPDATE forum_posts
                SET {', '.join(update_fields)}
                WHERE post_id = ?
                """

                with self._get_db_connection() as conn:
                    cursor = conn.execute(sql, values)
                    if cursor.rowcount > 0:
                        conn.commit()
                        # æ¸…é™¤ç¼“å­˜ï¼Œå¼ºåˆ¶ä¸‹æ¬¡ä»æ•°æ®åº“é‡æ–°åŠ è½½
                        self._clear_cache(post_id)
                        self.logger.info(f"å¸–å­çŠ¶æ€æ›´æ–°æˆåŠŸ: {post_id} -> {status}")
                        return True

                return False
        except Exception as e:
            self.logger.error(f"æ›´æ–°å¸–å­çŠ¶æ€å¤±è´¥: {e}")
            return False

    def _clear_cache(self, post_id: str):
        """æ¸…é™¤æŒ‡å®šå¸–å­çš„ç¼“å­˜"""
        if not self.redis_client:
            return

        try:
            cache_key = self._get_cache_key(post_id)
            self.redis_client.delete(cache_key)
        except Exception as e:
            self.logger.warning(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            with self._get_db_connection() as conn:
                # åŸºæœ¬ç»Ÿè®¡
                cursor = conn.execute("""
                    SELECT
                        processing_status,
                        COUNT(*) as count
                    FROM forum_posts
                    GROUP BY processing_status
                """)
                status_stats = {row[0]: row[1] for row in cursor.fetchall()}

                # æ€»æ•°ç»Ÿè®¡
                cursor = conn.execute("SELECT COUNT(*) FROM forum_posts")
                total_posts = cursor.fetchone()[0]

                # ä»Šæ—¥ç»Ÿè®¡
                today = datetime.now().date().isoformat()
                cursor = conn.execute(
                    "SELECT COUNT(*) FROM forum_posts WHERE DATE(discovered_time) = ?",
                    (today,)
                )
                today_posts = cursor.fetchone()[0]

                return {
                    'total_posts': total_posts,
                    'today_posts': today_posts,
                    'status_breakdown': status_stats,
                    'cache_enabled': self.redis_client is not None,
                    'last_updated': datetime.now().isoformat()
                }
        except Exception as e:
            self.logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    def cleanup_old_data(self, days: int = 30) -> int:
        """æ¸…ç†æ—§æ•°æ®"""
        try:
            cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()

            with self._get_db_connection() as conn:
                # æ¸…ç†å·²å®Œæˆçš„æ—§å¸–å­
                cursor = conn.execute("""
                    DELETE FROM forum_posts
                    WHERE processing_status = 'completed'
                    AND last_updated < ?
                """, (cutoff_date,))

                deleted_count = cursor.rowcount
                conn.commit()

                self.logger.info(f"æ¸…ç†äº† {deleted_count} æ¡æ—§æ•°æ®")
                return deleted_count

        except Exception as e:
            self.logger.error(f"æ¸…ç†æ—§æ•°æ®å¤±è´¥: {e}")
            return 0

    def export_data(self, output_path: str, format: str = 'json') -> bool:
        """å¯¼å‡ºæ•°æ®"""
        try:
            with self._get_db_connection() as conn:
                cursor = conn.execute("SELECT * FROM forum_posts ORDER BY discovered_time DESC")
                posts_data = []

                for row in cursor.fetchall():
                    post_data = dict(row)
                    # ç§»é™¤æ•°æ®åº“çš„idå­—æ®µä»¥ä¿æŒä¸€è‡´æ€§
                    if 'id' in post_data:
                        del post_data['id']
                    posts_data.append(post_data)

                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(output_path), exist_ok=True)

                if format.lower() == 'json':
                    with open(output_path, 'w', encoding='utf-8') as f:
                        json.dump(posts_data, f, ensure_ascii=False, indent=2, default=str)
                elif format.lower() == 'csv':
                    import csv
                    if posts_data:
                        with open(output_path, 'w', newline='', encoding='utf-8') as f:
                            writer = csv.DictWriter(f, fieldnames=posts_data[0].keys())
                            writer.writeheader()
                            writer.writerows(posts_data)

                self.logger.info(f"æ•°æ®å¯¼å‡ºæˆåŠŸ: {output_path}")
                return True

        except Exception as e:
            self.logger.error(f"æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
            return False

    def close(self):
        """å…³é—­è¿æ¥"""
        if self.redis_client:
            try:
                self.redis_client.close()
            except:
                pass
        self.logger.info("æ•°æ®ç®¡ç†å™¨å·²å…³é—­")


# å…¨å±€å®ä¾‹
_data_manager = None


def get_data_manager(db_path: str = "data/forum_posts.db") -> HybridForumDataManager:
    """è·å–æ•°æ®ç®¡ç†å™¨å•ä¾‹"""
    global _data_manager
    if _data_manager is None:
        _data_manager = HybridForumDataManager(db_path)
    return _data_manager


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª æµ‹è¯•æ··åˆæ•°æ®ç®¡ç†å™¨...")

    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    manager = HybridForumDataManager("data/test_forum_posts.db")

    # åˆ›å»ºæµ‹è¯•å¸–å­
    test_post = ForumPost(
        post_id="test_001",
        thread_id="thread_001",
        title="æµ‹è¯•å¸–å­",
        author_id="user_001",
        author_name="æµ‹è¯•ç”¨æˆ·",
        video_urls=["http://example.com/video1.mp4"],
        priority=2,
        post_time=datetime.now()  # æ·»åŠ å¿…éœ€çš„post_timeå­—æ®µ
    )

    # æµ‹è¯•ä¿å­˜
    if manager.save_post(test_post):
        print("âœ… å¸–å­ä¿å­˜æˆåŠŸ")

    # æµ‹è¯•è·å–
    retrieved_post = manager.get_post("test_001")
    if retrieved_post:
        print(f"âœ… å¸–å­è·å–æˆåŠŸ: {retrieved_post.title}")

    # æµ‹è¯•ç»Ÿè®¡
    stats = manager.get_statistics()
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}")

    # æ¸…ç†
    manager.close()
    print("ğŸ‰ æµ‹è¯•å®Œæˆ")
