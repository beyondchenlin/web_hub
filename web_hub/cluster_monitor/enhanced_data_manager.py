#!/usr/bin/env python3
"""
SQLite + Redis åŒå±‚å­˜å‚¨æ•°æ®ç®¡ç†å™¨
ä¸“ä¸ºcluster_monitorè®¾è®¡çš„ä¼ä¸šçº§æ•°æ®å­˜å‚¨æ–¹æ¡ˆ

ç‰¹ç‚¹ï¼š
1. SQLiteæŒä¹…åŒ–å­˜å‚¨ - æ•°æ®å®‰å…¨ä¿éšœ
2. Redisé«˜é€Ÿç¼“å­˜ - æå‡è®¿é—®æ€§èƒ½
3. è‡ªåŠ¨æ•°æ®åŒæ­¥ - ä¿è¯æ•°æ®ä¸€è‡´æ€§
4. æ•…éšœæ¢å¤æœºåˆ¶ - Redisä¸å¯ç”¨æ—¶é™çº§åˆ°SQLite
5. å®Œæ•´çš„ç»Ÿè®¡å’ŒæŸ¥è¯¢åŠŸèƒ½
"""

import os
import json
import sqlite3
import threading
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, asdict
import logging

# Redisæ”¯æŒæ£€æµ‹
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    print("âš ï¸ Redisä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨çº¯SQLiteæ¨¡å¼")


@dataclass
class ForumPostRecord:
    """è®ºå›å¸–å­è®°å½•ï¼ˆä¸æ•°æ®åº“è¡¨ç»“æ„ä¸€è‡´ï¼‰"""
    post_id: str
    thread_id: str
    title: str
    author_name: str
    source_url: str  # ç»Ÿä¸€ä½¿ç”¨ source_urlï¼ˆä¸æ•°æ®åº“è¡¨å’Œå·¥ä½œèŠ‚ç‚¹ä¸€è‡´ï¼‰
    discovered_time: datetime
    processing_status: str = "discovered"  # discovered, dispatched, completed, failed
    dispatch_time: Optional[datetime] = None
    completion_time: Optional[datetime] = None
    task_id: Optional[str] = None
    machine_url: Optional[str] = None
    error_message: Optional[str] = None
    retry_count: int = 0
    has_video: bool = False
    has_audio: bool = False
    content_length: int = 0
    tags: List[str] = None
    created_at: Optional[datetime] = None
    last_updated: Optional[datetime] = None

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        data = asdict(self)
        # å¤„ç†datetimeå¯¹è±¡
        for key, value in data.items():
            if isinstance(value, datetime):
                data[key] = value.isoformat()
        # å¤„ç†åˆ—è¡¨
        if data['tags'] is None:
            data['tags'] = []
        return data

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ForumPostRecord':
        """ä»å­—å…¸åˆ›å»ºå¯¹è±¡"""
        # å¤„ç†datetimeå­—æ®µ
        datetime_fields = ['discovered_time', 'dispatch_time', 'completion_time', 'created_at', 'last_updated']
        for field in datetime_fields:
            if data.get(field):
                data[field] = datetime.fromisoformat(data[field])

        # å¤„ç†tagså­—æ®µ
        if data.get('tags') is None:
            data['tags'] = []

        # è¿‡æ»¤æ‰ä¸å­˜åœ¨çš„å­—æ®µï¼Œé¿å…æ„å¤–çš„å…³é”®å­—å‚æ•°é”™è¯¯
        valid_fields = {
            'post_id', 'thread_id', 'title', 'author_name', 'source_url',
            'discovered_time', 'processing_status', 'dispatch_time',
            'completion_time', 'task_id', 'machine_url', 'error_message',
            'retry_count', 'has_video', 'has_audio', 'content_length',
            'tags', 'created_at', 'last_updated'
        }

        filtered_data = {k: v for k, v in data.items() if k in valid_fields}

        return cls(**filtered_data)


class SQLiteRedisDataManager:
    """SQLite + Redis åŒå±‚å­˜å‚¨æ•°æ®ç®¡ç†å™¨"""

    def __init__(self, db_path: str = "data/forum_posts.db",
                 redis_host: str = "localhost", redis_port: int = 6379, redis_db: int = 1):
        self.db_path = db_path
        self.redis_client = None
        self._lock = threading.RLock()
        self.logger = logging.getLogger(__name__)

        # Redisé…ç½®
        self.redis_config = {
            'host': redis_host,
            'port': redis_port,
            'db': redis_db,
            'decode_responses': True,
            'socket_timeout': 5,
            'socket_connect_timeout': 5
        }

        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(db_path), exist_ok=True)

        # åˆå§‹åŒ–SQLiteæ•°æ®åº“
        self._init_database()

        # åˆå§‹åŒ–Redisè¿æ¥
        self._init_redis()

        # ç»Ÿè®¡ä¿¡æ¯ç¼“å­˜
        self._stats_cache = {}
        self._stats_cache_time = None

        print(f"ğŸ“Š SQLite + Redis æ•°æ®ç®¡ç†å™¨å·²åˆå§‹åŒ–")
        print(f"   SQLite: {self.db_path}")
        print(f"   Redis: {'âœ… å¯ç”¨' if self.redis_client else 'âŒ ä¸å¯ç”¨ï¼Œä½¿ç”¨SQLiteæ¨¡å¼'}")
        
    def _init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“ - ä½¿ç”¨ç»Ÿä¸€çš„SQLè„šæœ¬"""
        try:
            # å°è¯•ä½¿ç”¨ç»Ÿä¸€çš„ forum_posts.sql è„šæœ¬ï¼ˆä¸å·¥ä½œèŠ‚ç‚¹ç›¸åŒï¼‰
            base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            candidate_paths = [
                os.path.join(base_dir, "forum_posts.sql"),                # web_hub/forum_posts.sql
                os.path.join(os.getcwd(), "web_hub", "forum_posts.sql"), # ä»ä»“åº“æ ¹ç›®å½•è¿è¡Œ
                os.path.join(os.getcwd(), "forum_posts.sql"),             # å½“å‰ç›®å½•
            ]

            sql_script_path = next((p for p in candidate_paths if os.path.exists(p)), None)

            if sql_script_path:
                # ä½¿ç”¨ç»Ÿä¸€çš„SQLè„šæœ¬åˆ›å»ºè¡¨
                self.logger.info(f"ä½¿ç”¨ç»Ÿä¸€SQLè„šæœ¬åˆå§‹åŒ–æ•°æ®åº“: {sql_script_path}")
                with open(sql_script_path, 'r', encoding='utf-8') as f:
                    sql_script = f.read()

                with sqlite3.connect(self.db_path) as conn:
                    conn.executescript(sql_script)
                    conn.commit()

                self.logger.info("SQLiteæ•°æ®åº“åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨ç»Ÿä¸€è„šæœ¬ï¼‰")
            else:
                # é™çº§æ–¹æ¡ˆï¼šæ‰‹åŠ¨åˆ›å»ºè¡¨ï¼ˆä¸forum_posts.sqlä¿æŒä¸€è‡´ï¼‰
                self.logger.warning("æœªæ‰¾åˆ°forum_posts.sqlï¼Œä½¿ç”¨å†…ç½®è¡¨ç»“æ„")
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute("""
                    CREATE TABLE IF NOT EXISTS forum_posts (
                        post_id TEXT PRIMARY KEY,
                        thread_id TEXT NOT NULL,
                        title TEXT NOT NULL,
                        author_name TEXT NOT NULL,
                        source_url TEXT NOT NULL,
                        discovered_time TEXT NOT NULL,
                        processing_status TEXT DEFAULT 'discovered',
                        dispatch_time TEXT,
                        completion_time TEXT,
                        task_id TEXT,
                        machine_url TEXT,
                        error_message TEXT,
                        retry_count INTEGER DEFAULT 0,
                        has_video BOOLEAN DEFAULT FALSE,
                        has_audio BOOLEAN DEFAULT FALSE,
                        content_length INTEGER DEFAULT 0,
                        tags TEXT,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                        last_updated TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                    """)
                    conn.commit()

            # å…¼å®¹æ€§æ£€æŸ¥ï¼šå¦‚æœæ˜¯æ—§æ•°æ®åº“ï¼Œæ·»åŠ ç¼ºå¤±çš„ç›‘æ§å­—æ®µ
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("PRAGMA table_info(forum_posts)")
                existing_columns = {row[1] for row in cursor.fetchall()}

                # ç›‘æ§èŠ‚ç‚¹å¿…éœ€çš„å­—æ®µ
                required_fields = {
                    'machine_url': 'TEXT',
                    'dispatch_time': 'TEXT',
                    'completion_time': 'TEXT',
                    'error_message': 'TEXT',
                    'retry_count': 'INTEGER DEFAULT 0',
                    'has_video': 'BOOLEAN DEFAULT FALSE',
                    'has_audio': 'BOOLEAN DEFAULT FALSE',
                    'content_length': 'INTEGER DEFAULT 0',
                    'tags': 'TEXT',
                    'created_at': 'TEXT DEFAULT CURRENT_TIMESTAMP',
                }

                # æ·»åŠ ç¼ºå¤±çš„å­—æ®µï¼ˆå…¼å®¹æ—§æ•°æ®åº“ï¼‰
                for field_name, field_type in required_fields.items():
                    if field_name not in existing_columns:
                        try:
                            conn.execute(f"ALTER TABLE forum_posts ADD COLUMN {field_name} {field_type}")
                            self.logger.info(f"æ·»åŠ ç›‘æ§å­—æ®µ: {field_name}")
                        except Exception as e:
                            self.logger.warning(f"æ·»åŠ å­—æ®µ {field_name} å¤±è´¥ï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰: {e}")

                conn.commit()

                # åˆ›å»ºç´¢å¼•
                conn.execute("CREATE INDEX IF NOT EXISTS idx_status ON forum_posts(processing_status)")
                conn.execute("CREATE INDEX IF NOT EXISTS idx_discovered_time ON forum_posts(discovered_time)")
                conn.execute("CREATE INDEX IF NOT EXISTS idx_author ON forum_posts(author_name)")
                conn.execute("CREATE INDEX IF NOT EXISTS idx_machine ON forum_posts(machine_url)")
                conn.execute("CREATE INDEX IF NOT EXISTS idx_retry ON forum_posts(retry_count)")

                conn.commit()
                self.logger.info("SQLiteæ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            self.logger.error(f"SQLiteæ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _init_redis(self):
        """åˆå§‹åŒ–Redisè¿æ¥"""
        if not REDIS_AVAILABLE:
            self.logger.warning("Redisæ¨¡å—ä¸å¯ç”¨")
            return

        try:
            self.redis_client = redis.Redis(**self.redis_config)
            self.redis_client.ping()
            self.logger.info("Redisè¿æ¥æˆåŠŸ")

            # è®¾ç½®Redisé”®å‰ç¼€
            self.redis_prefix = "forum_monitor:"

        except Exception as e:
            self.logger.warning(f"Redisè¿æ¥å¤±è´¥: {e}ï¼Œå°†åªä½¿ç”¨SQLiteå­˜å‚¨")
            self.redis_client = None
    
    def save_post(self, post: ForumPostRecord) -> bool:
        """ä¿å­˜å¸–å­è®°å½•"""
        try:
            with self._lock:
                # ä¿å­˜åˆ°SQLiteï¼ˆä¸»å­˜å‚¨ï¼‰
                success = self._save_to_sqlite(post)
                if success:
                    # æ›´æ–°Redisç¼“å­˜
                    self._update_redis_cache(post)
                    # æ›´æ–°RedisçŠ¶æ€é›†åˆ
                    self._update_redis_status_sets(post)
                    # æ¸…é™¤ç»Ÿè®¡ç¼“å­˜
                    self._clear_stats_cache()

                return success
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¸–å­è®°å½•å¤±è´¥: {e}")
            return False
    
    def _save_to_sqlite(self, post: ForumPostRecord) -> bool:
        """ä¿å­˜åˆ°SQLite"""
        try:
            data = post.to_dict()
            data['tags'] = json.dumps(data['tags'], ensure_ascii=False)
            data['last_updated'] = datetime.now().isoformat()

            # ç§»é™¤ä¸å­˜åœ¨çš„å­—æ®µï¼ˆç»Ÿä¸€ä½¿ç”¨ source_urlï¼‰
            valid_columns = [
                'post_id', 'thread_id', 'title', 'author_name', 'source_url',
                'discovered_time', 'processing_status', 'dispatch_time',
                'completion_time', 'task_id', 'machine_url', 'error_message',
                'retry_count', 'has_video', 'has_audio', 'content_length',
                'tags', 'created_at', 'last_updated'
            ]

            filtered_data = {k: v for k, v in data.items() if k in valid_columns}
            columns = list(filtered_data.keys())
            placeholders = ['?' for _ in columns]
            values = list(filtered_data.values())

            sql = f"""
            INSERT OR REPLACE INTO forum_posts ({', '.join(columns)})
            VALUES ({', '.join(placeholders)})
            """

            with sqlite3.connect(self.db_path) as conn:
                conn.execute(sql, values)
                conn.commit()

            return True
        except Exception as e:
            self.logger.error(f"SQLiteä¿å­˜å¤±è´¥: {e}")
            return False

    def _update_redis_cache(self, post: ForumPostRecord):
        """æ›´æ–°Redisç¼“å­˜"""
        try:
            if not self.redis_client:
                return

            cache_key = f"{self.redis_prefix}post:{post.post_id}"
            post_json = json.dumps(post.to_dict(), ensure_ascii=False)

            # è®¾ç½®ç¼“å­˜ï¼Œè¿‡æœŸæ—¶é—´24å°æ—¶
            self.redis_client.setex(cache_key, 86400, post_json)

        except Exception as e:
            self.logger.warning(f"Redisç¼“å­˜æ›´æ–°å¤±è´¥: {e}")

    def _update_redis_status_sets(self, post: ForumPostRecord):
        """æ›´æ–°RedisçŠ¶æ€é›†åˆ"""
        try:
            if not self.redis_client:
                return

            # ä»æ‰€æœ‰çŠ¶æ€é›†åˆä¸­ç§»é™¤
            for status in ['discovered', 'dispatched', 'completed', 'failed']:
                status_key = f"{self.redis_prefix}status:{status}"
                self.redis_client.srem(status_key, post.post_id)

            # æ·»åŠ åˆ°å½“å‰çŠ¶æ€é›†åˆ
            current_status_key = f"{self.redis_prefix}status:{post.processing_status}"
            self.redis_client.sadd(current_status_key, post.post_id)

            # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆ7å¤©ï¼‰
            self.redis_client.expire(current_status_key, 604800)

        except Exception as e:
            self.logger.warning(f"RedisçŠ¶æ€é›†åˆæ›´æ–°å¤±è´¥: {e}")
    
    def get_post(self, post_id: str) -> Optional[ForumPostRecord]:
        """è·å–å¸–å­è®°å½•"""
        try:
            # å…ˆå°è¯•ä»Redisè·å–
            if self.redis_client:
                cached_data = self._get_from_redis(post_id)
                if cached_data:
                    return cached_data

            # ä»SQLiteè·å–
            post = self._get_from_sqlite(post_id)

            # å¦‚æœä»SQLiteè·å–æˆåŠŸï¼Œæ›´æ–°Redisç¼“å­˜
            if post and self.redis_client:
                self._update_redis_cache(post)

            return post

        except Exception as e:
            self.logger.error(f"è·å–å¸–å­è®°å½•å¤±è´¥: {e}")
            return None

    def _get_from_redis(self, post_id: str) -> Optional[ForumPostRecord]:
        """ä»Redisè·å–"""
        try:
            cache_key = f"{self.redis_prefix}post:{post_id}"
            cached_json = self.redis_client.get(cache_key)
            if cached_json:
                post_data = json.loads(cached_json)
                return ForumPostRecord.from_dict(post_data)
        except Exception as e:
            self.logger.warning(f"Redisè¯»å–å¤±è´¥: {e}")
        return None

    def _get_from_sqlite(self, post_id: str) -> Optional[ForumPostRecord]:
        """ä»SQLiteè·å–"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.execute("SELECT * FROM forum_posts WHERE post_id = ?", (post_id,))
                row = cursor.fetchone()

                if row:
                    data = dict(row)
                    # å¤„ç†tagså­—æ®µ
                    if data['tags']:
                        data['tags'] = json.loads(data['tags'])
                    else:
                        data['tags'] = []

                    return ForumPostRecord.from_dict(data)
        except Exception as e:
            self.logger.error(f"SQLiteè¯»å–å¤±è´¥: {e}")
        return None
    
    def is_post_processed(self, post_id: str) -> bool:
        """æ£€æŸ¥å¸–å­æ˜¯å¦å·²è¢«å¤„ç†"""
        try:
            # å…ˆæ£€æŸ¥RedisçŠ¶æ€é›†åˆï¼ˆå¿«é€ŸæŸ¥è¯¢ï¼‰
            if self.redis_client:
                for status in ['dispatched', 'completed']:
                    status_key = f"{self.redis_prefix}status:{status}"
                    if self.redis_client.sismember(status_key, post_id):
                        return True

            # ä»SQLiteæ£€æŸ¥
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute(
                    "SELECT 1 FROM forum_posts WHERE post_id = ? AND processing_status IN ('dispatched', 'completed')",
                    (post_id,)
                )
                return cursor.fetchone() is not None

        except Exception as e:
            self.logger.error(f"æ£€æŸ¥å¸–å­å¤„ç†çŠ¶æ€å¤±è´¥: {e}")
            return False

    def add_post(self, post_id: str, title: str, author: str, url: str) -> bool:
        """æ·»åŠ æ–°å¸–å­"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if self.get_post(post_id):
                return False

            post = ForumPostRecord(
                post_id=post_id,
                thread_id=post_id,  # ä½¿ç”¨post_idä½œä¸ºthread_id
                title=title,
                author_name=author,
                source_url=url,  # ç»Ÿä¸€ä½¿ç”¨ source_url
                discovered_time=datetime.now()
            )

            return self.save_post(post)

        except Exception as e:
            self.logger.error(f"æ·»åŠ å¸–å­å¤±è´¥: {e}")
            return False
    
    def mark_post_dispatched(self, post_id: str, machine_url: str) -> bool:
        """æ ‡è®°å¸–å­å·²åˆ†å‘"""
        return self.update_post_status(
            post_id,
            'dispatched',
            machine_url=machine_url,
            dispatch_time=datetime.now()
        )

    def mark_post_completed(self, post_id: str) -> bool:
        """æ ‡è®°å¸–å­å®Œæˆ"""
        return self.update_post_status(
            post_id,
            'completed',
            completion_time=datetime.now()
        )

    def mark_post_failed(self, post_id: str, error_message: str) -> bool:
        """æ ‡è®°å¸–å­å¤±è´¥"""
        post = self.get_post(post_id)
        retry_count = post.retry_count + 1 if post else 1

        return self.update_post_status(
            post_id,
            'failed',
            error_message=error_message,
            retry_count=retry_count
        )

    def get_posts_by_status(self, status: str, limit: int = 100) -> List[ForumPostRecord]:
        """æŒ‰çŠ¶æ€è·å–å¸–å­åˆ—è¡¨"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.execute(
                    """SELECT * FROM forum_posts
                       WHERE processing_status = ?
                       ORDER BY discovered_time DESC
                       LIMIT ?""",
                    (status, limit)
                )

                posts = []
                for row in cursor.fetchall():
                    data = dict(row)
                    if data['tags']:
                        data['tags'] = json.loads(data['tags'])
                    else:
                        data['tags'] = []
                    posts.append(ForumPostRecord.from_dict(data))

                return posts
        except Exception as e:
            self.logger.error(f"æŒ‰çŠ¶æ€æŸ¥è¯¢å¸–å­å¤±è´¥: {e}")
            return []
    
    def update_post_status(self, post_id: str, status: str, **kwargs) -> bool:
        """æ›´æ–°å¸–å­çŠ¶æ€"""
        try:
            with self._lock:
                # æ„å»ºæ›´æ–°å­—æ®µ
                update_fields = ["processing_status = ?", "last_updated = ?"]
                values = [status, datetime.now().isoformat()]

                # æ·»åŠ å…¶ä»–å­—æ®µ
                for key, value in kwargs.items():
                    if key in ['dispatch_time', 'completion_time'] and isinstance(value, datetime):
                        value = value.isoformat()
                    update_fields.append(f"{key} = ?")
                    values.append(value)

                values.append(post_id)

                sql = f"""
                UPDATE forum_posts
                SET {', '.join(update_fields)}
                WHERE post_id = ?
                """

                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.execute(sql, values)
                    success = cursor.rowcount > 0
                    conn.commit()

                if success:
                    # è·å–æ›´æ–°åçš„å¸–å­ä¿¡æ¯
                    updated_post = self._get_from_sqlite(post_id)
                    if updated_post:
                        # æ›´æ–°Redisç¼“å­˜
                        self._update_redis_cache(updated_post)
                        # æ›´æ–°RedisçŠ¶æ€é›†åˆ
                        self._update_redis_status_sets(updated_post)

                    # æ¸…é™¤ç»Ÿè®¡ç¼“å­˜
                    self._clear_stats_cache()

                return success
        except Exception as e:
            self.logger.error(f"æ›´æ–°å¸–å­çŠ¶æ€å¤±è´¥: {e}")
            return False

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            if (self._stats_cache_time and
                datetime.now() - self._stats_cache_time < timedelta(minutes=5)):
                return self._stats_cache

            with sqlite3.connect(self.db_path) as conn:
                # çŠ¶æ€ç»Ÿè®¡
                cursor = conn.execute("""
                    SELECT processing_status, COUNT(*) as count
                    FROM forum_posts
                    GROUP BY processing_status
                """)
                status_counts = dict(cursor.fetchall())

                # æ€»æ•°ç»Ÿè®¡
                cursor = conn.execute("SELECT COUNT(*) FROM forum_posts")
                total_posts = cursor.fetchone()[0]

                # ä»Šæ—¥ç»Ÿè®¡
                today = datetime.now().date().isoformat()
                cursor = conn.execute("""
                    SELECT COUNT(*) FROM forum_posts
                    WHERE DATE(discovered_time) = ?
                """, (today,))
                today_posts = cursor.fetchone()[0]

                # æœºå™¨ç»Ÿè®¡
                cursor = conn.execute("""
                    SELECT machine_url, COUNT(*) as count
                    FROM forum_posts
                    WHERE machine_url IS NOT NULL
                    GROUP BY machine_url
                """)
                machine_stats = dict(cursor.fetchall())

                stats = {
                    'total_posts': total_posts,
                    'today_posts': today_posts,
                    'status_counts': status_counts,
                    'machine_stats': machine_stats,
                    'processed_posts': status_counts.get('dispatched', 0) + status_counts.get('completed', 0),
                    'redis_available': self.redis_client is not None,
                    'last_updated': datetime.now().isoformat()
                }

                # ç¼“å­˜ç»“æœ
                self._stats_cache = stats
                self._stats_cache_time = datetime.now()

                return stats

        except Exception as e:
            self.logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    def _clear_stats_cache(self):
        """æ¸…é™¤ç»Ÿè®¡ç¼“å­˜"""
        self._stats_cache = {}
        self._stats_cache_time = None

    def close(self):
        """å…³é—­æ•°æ®ç®¡ç†å™¨"""
        try:
            if self.redis_client:
                self.redis_client.close()
            print("ğŸ’¾ SQLite + Redis æ•°æ®ç®¡ç†å™¨å·²å…³é—­")
        except Exception as e:
            self.logger.error(f"å…³é—­æ•°æ®ç®¡ç†å™¨å¤±è´¥: {e}")


# å…¨å±€å®ä¾‹
_data_manager = None


def get_sqlite_redis_data_manager() -> SQLiteRedisDataManager:
    """è·å–SQLite + Redisæ•°æ®ç®¡ç†å™¨å•ä¾‹"""
    global _data_manager
    if _data_manager is None:
        _data_manager = SQLiteRedisDataManager()
    return _data_manager
