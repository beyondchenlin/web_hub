#!/usr/bin/env python3
"""
ç‹¬ç«‹ç›‘æ§ç³»ç»Ÿæ•°æ®ç®¡ç†å™¨
ä¸“ä¸ºcluster_monitorè®¾è®¡çš„è½»é‡çº§ã€æ— å¤–éƒ¨ä¾èµ–çš„æ•°æ®å­˜å‚¨æ–¹æ¡ˆ

ç‰¹ç‚¹ï¼š
1. çº¯Pythonæ ‡å‡†åº“å®ç°ï¼Œæ— éœ€Redis/SQLite
2. é«˜æ€§èƒ½å†…å­˜å­˜å‚¨ + å®šæœŸæŒä¹…åŒ–
3. è‡ªåŠ¨æ•°æ®æ¸…ç†å’Œç»Ÿè®¡åŠŸèƒ½
4. æ”¯æŒå¹¶å‘è®¿é—®å’Œæ•…éšœæ¢å¤
"""

import os
import json
import threading
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import logging


@dataclass
class PostRecord:
    """å¸–å­è®°å½•"""
    post_id: str
    title: str
    author: str
    url: str
    discovered_time: datetime
    status: str = "discovered"  # discovered, dispatched, completed, failed
    machine_url: Optional[str] = None
    dispatch_time: Optional[datetime] = None
    completion_time: Optional[datetime] = None
    error_message: Optional[str] = None
    retry_count: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        data = asdict(self)
        # å¤„ç†datetimeå¯¹è±¡
        for key, value in data.items():
            if isinstance(value, datetime):
                data[key] = value.isoformat()
        return data
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'PostRecord':
        """ä»å­—å…¸åˆ›å»ºå¯¹è±¡"""
        # å¤„ç†datetimeå­—æ®µ
        datetime_fields = ['discovered_time', 'dispatch_time', 'completion_time']
        for field in datetime_fields:
            if data.get(field):
                data[field] = datetime.fromisoformat(data[field])
        return cls(**data)


class StandaloneDataManager:
    """ç‹¬ç«‹æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, data_dir: str = "data", max_records: int = 10000):
        self.data_dir = data_dir
        self.max_records = max_records
        self.data_file = os.path.join(data_dir, "posts_data.json")
        self.stats_file = os.path.join(data_dir, "stats.json")
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(data_dir, exist_ok=True)
        
        # å†…å­˜å­˜å‚¨
        self._posts: Dict[str, PostRecord] = {}
        self._processed_ids: set = set()  # å¿«é€ŸæŸ¥æ‰¾å·²å¤„ç†çš„å¸–å­ID
        self._stats = {
            'total_discovered': 0,
            'total_dispatched': 0,
            'total_completed': 0,
            'total_failed': 0,
            'start_time': datetime.now().isoformat(),
            'last_cleanup': datetime.now().isoformat()
        }
        
        # çº¿ç¨‹é”
        self._lock = threading.RLock()
        
        # è‡ªåŠ¨ä¿å­˜é…ç½®
        self._auto_save_interval = 60  # 60ç§’è‡ªåŠ¨ä¿å­˜ä¸€æ¬¡
        self._last_save_time = time.time()
        
        # æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½æ•°æ®
        self._load_data()
        
        # å¯åŠ¨åå°ä»»åŠ¡
        self._start_background_tasks()
        
        print(f"ğŸ“Š ç‹¬ç«‹æ•°æ®ç®¡ç†å™¨å·²åˆå§‹åŒ–")
        print(f"   æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"   å·²åŠ è½½è®°å½•: {len(self._posts)}")
        print(f"   å·²å¤„ç†å¸–å­: {len(self._processed_ids)}")
    
    def _load_data(self):
        """åŠ è½½æ•°æ®"""
        try:
            # åŠ è½½å¸–å­æ•°æ®
            if os.path.exists(self.data_file):
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for post_data in data.get('posts', []):
                        post = PostRecord.from_dict(post_data)
                        self._posts[post.post_id] = post
                        if post.status in ['dispatched', 'completed']:
                            self._processed_ids.add(post.post_id)
                
                print(f"ğŸ’¾ åŠ è½½äº† {len(self._posts)} ä¸ªå¸–å­è®°å½•")
            
            # åŠ è½½ç»Ÿè®¡æ•°æ®
            if os.path.exists(self.stats_file):
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    saved_stats = json.load(f)
                    self._stats.update(saved_stats)
                
        except Exception as e:
            self.logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
    
    def _save_data(self):
        """ä¿å­˜æ•°æ®"""
        try:
            with self._lock:
                # ä¿å­˜å¸–å­æ•°æ®
                posts_data = {
                    'posts': [post.to_dict() for post in self._posts.values()],
                    'saved_at': datetime.now().isoformat()
                }
                
                with open(self.data_file, 'w', encoding='utf-8') as f:
                    json.dump(posts_data, f, ensure_ascii=False, indent=2)
                
                # ä¿å­˜ç»Ÿè®¡æ•°æ®
                self._stats['last_save'] = datetime.now().isoformat()
                with open(self.stats_file, 'w', encoding='utf-8') as f:
                    json.dump(self._stats, f, ensure_ascii=False, indent=2)
                
                self._last_save_time = time.time()
                
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    
    def is_post_processed(self, post_id: str) -> bool:
        """æ£€æŸ¥å¸–å­æ˜¯å¦å·²è¢«å¤„ç†"""
        return post_id in self._processed_ids
    
    def add_post(self, post_id: str, title: str, author: str, url: str) -> bool:
        """æ·»åŠ æ–°å¸–å­"""
        try:
            with self._lock:
                if post_id in self._posts:
                    return False  # å·²å­˜åœ¨
                
                post = PostRecord(
                    post_id=post_id,
                    title=title,
                    author=author,
                    url=url,
                    discovered_time=datetime.now()
                )
                
                self._posts[post_id] = post
                self._stats['total_discovered'] += 1
                
                # è‡ªåŠ¨ä¿å­˜æ£€æŸ¥
                self._check_auto_save()
                
                return True
        except Exception as e:
            self.logger.error(f"æ·»åŠ å¸–å­å¤±è´¥: {e}")
            return False
    
    def mark_post_dispatched(self, post_id: str, machine_url: str) -> bool:
        """æ ‡è®°å¸–å­å·²åˆ†å‘"""
        try:
            with self._lock:
                if post_id not in self._posts:
                    return False
                
                post = self._posts[post_id]
                post.status = "dispatched"
                post.machine_url = machine_url
                post.dispatch_time = datetime.now()
                
                self._processed_ids.add(post_id)
                self._stats['total_dispatched'] += 1
                
                return True
        except Exception as e:
            self.logger.error(f"æ ‡è®°å¸–å­åˆ†å‘å¤±è´¥: {e}")
            return False
    
    def mark_post_completed(self, post_id: str) -> bool:
        """æ ‡è®°å¸–å­å®Œæˆ"""
        try:
            with self._lock:
                if post_id not in self._posts:
                    return False
                
                post = self._posts[post_id]
                post.status = "completed"
                post.completion_time = datetime.now()
                
                self._stats['total_completed'] += 1
                
                return True
        except Exception as e:
            self.logger.error(f"æ ‡è®°å¸–å­å®Œæˆå¤±è´¥: {e}")
            return False
    
    def mark_post_failed(self, post_id: str, error_message: str) -> bool:
        """æ ‡è®°å¸–å­å¤±è´¥"""
        try:
            with self._lock:
                if post_id not in self._posts:
                    return False
                
                post = self._posts[post_id]
                post.status = "failed"
                post.error_message = error_message
                post.retry_count += 1
                
                self._stats['total_failed'] += 1
                
                return True
        except Exception as e:
            self.logger.error(f"æ ‡è®°å¸–å­å¤±è´¥: {e}")
            return False
    
    def get_posts_by_status(self, status: str, limit: int = 100) -> List[PostRecord]:
        """æŒ‰çŠ¶æ€è·å–å¸–å­"""
        try:
            with self._lock:
                posts = [post for post in self._posts.values() if post.status == status]
                # æŒ‰å‘ç°æ—¶é—´æ’åº
                posts.sort(key=lambda x: x.discovered_time, reverse=True)
                return posts[:limit]
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢å¸–å­å¤±è´¥: {e}")
            return []
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            with self._lock:
                # è®¡ç®—å®æ—¶ç»Ÿè®¡
                status_counts = defaultdict(int)
                for post in self._posts.values():
                    status_counts[post.status] += 1
                
                # è®¡ç®—è¿è¡Œæ—¶é—´
                start_time = datetime.fromisoformat(self._stats['start_time'])
                uptime_seconds = (datetime.now() - start_time).total_seconds()
                
                return {
                    'total_posts': len(self._posts),
                    'processed_posts': len(self._processed_ids),
                    'status_counts': dict(status_counts),
                    'uptime_seconds': int(uptime_seconds),
                    'memory_usage_mb': self._estimate_memory_usage(),
                    'last_save': self._stats.get('last_save', 'Never'),
                    **self._stats
                }
        except Exception as e:
            self.logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def _estimate_memory_usage(self) -> float:
        """ä¼°ç®—å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        try:
            import sys
            total_size = 0
            total_size += sys.getsizeof(self._posts)
            total_size += sys.getsizeof(self._processed_ids)
            for post in self._posts.values():
                total_size += sys.getsizeof(post)
            return round(total_size / 1024 / 1024, 2)
        except:
            return 0.0
    
    def _check_auto_save(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨ä¿å­˜"""
        if time.time() - self._last_save_time > self._auto_save_interval:
            self._save_data()
    
    def _start_background_tasks(self):
        """å¯åŠ¨åå°ä»»åŠ¡"""
        def background_worker():
            while True:
                try:
                    time.sleep(300)  # 5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
                    self._cleanup_old_records()
                    self._save_data()
                except Exception as e:
                    self.logger.error(f"åå°ä»»åŠ¡å¼‚å¸¸: {e}")
        
        thread = threading.Thread(target=background_worker, daemon=True)
        thread.start()
    
    def _cleanup_old_records(self):
        """æ¸…ç†æ—§è®°å½•"""
        try:
            with self._lock:
                if len(self._posts) <= self.max_records:
                    return
                
                # æŒ‰æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„è®°å½•
                posts_by_time = sorted(
                    self._posts.values(),
                    key=lambda x: x.discovered_time,
                    reverse=True
                )
                
                # ä¿ç•™æœ€æ–°çš„è®°å½•
                posts_to_keep = posts_by_time[:self.max_records]
                new_posts = {post.post_id: post for post in posts_to_keep}
                
                # æ›´æ–°å†…å­˜æ•°æ®
                removed_count = len(self._posts) - len(new_posts)
                self._posts = new_posts
                
                # é‡å»ºå·²å¤„ç†IDé›†åˆ
                self._processed_ids = {
                    post.post_id for post in new_posts.values()
                    if post.status in ['dispatched', 'completed']
                }
                
                self._stats['last_cleanup'] = datetime.now().isoformat()
                
                if removed_count > 0:
                    print(f"ğŸ§¹ æ¸…ç†äº† {removed_count} ä¸ªæ—§è®°å½•")
                
        except Exception as e:
            self.logger.error(f"æ¸…ç†æ—§è®°å½•å¤±è´¥: {e}")
    
    def force_save(self):
        """å¼ºåˆ¶ä¿å­˜æ•°æ®"""
        self._save_data()
    
    def close(self):
        """å…³é—­æ•°æ®ç®¡ç†å™¨"""
        self._save_data()
        print("ğŸ’¾ æ•°æ®å·²ä¿å­˜ï¼Œç®¡ç†å™¨å·²å…³é—­")


# å…¨å±€å®ä¾‹
_data_manager = None


def get_standalone_data_manager() -> StandaloneDataManager:
    """è·å–ç‹¬ç«‹æ•°æ®ç®¡ç†å™¨å•ä¾‹"""
    global _data_manager
    if _data_manager is None:
        _data_manager = StandaloneDataManager()
    return _data_manager


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª æµ‹è¯•ç‹¬ç«‹æ•°æ®ç®¡ç†å™¨...")
    
    manager = StandaloneDataManager("test_data")
    
    # æµ‹è¯•æ·»åŠ å¸–å­
    manager.add_post("test_001", "æµ‹è¯•å¸–å­1", "ç”¨æˆ·1", "http://example.com/1")
    manager.add_post("test_002", "æµ‹è¯•å¸–å­2", "ç”¨æˆ·2", "http://example.com/2")
    
    # æµ‹è¯•æ ‡è®°çŠ¶æ€
    manager.mark_post_dispatched("test_001", "http://localhost:8003")
    manager.mark_post_completed("test_001")
    
    # æµ‹è¯•æŸ¥è¯¢
    print(f"å·²å¤„ç†: {manager.is_post_processed('test_001')}")
    print(f"æœªå¤„ç†: {manager.is_post_processed('test_002')}")
    
    # æµ‹è¯•ç»Ÿè®¡
    stats = manager.get_statistics()
    print(f"ç»Ÿè®¡ä¿¡æ¯: {stats}")
    
    manager.close()
    print("ğŸ‰ æµ‹è¯•å®Œæˆ")
